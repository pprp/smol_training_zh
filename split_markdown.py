#!/usr/bin/env python3
"""
Script to split a large markdown file into semantic sections based on headings and content structure.
"""

import re
import os
from pathlib import Path

def split_markdown_into_sections(file_path, output_dir="sections"):
    """
    Split a markdown file into semantic sections based on major headings.

    Args:
        file_path (str): Path to the input markdown file
        output_dir (str): Directory to save the split sections
    """

    # Create output directory
    Path(output_dir).mkdir(exist_ok=True)

    # Read the entire file
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Split content into lines
    lines = content.split('\n')

    # Define major section markers based on the analysis
    major_sections = [
        ("introduction", "å¼•è¨€", 1),
        ("training-compass", "è®­ç»ƒæŒ‡å—é’ˆ", 36),
        ("why-question", "Whyï¼šæ²¡äººæ„¿æ„å›ç­”çš„é—®é¢˜", 51),
        ("what-decisions", "Whatï¼šå°†ç›®æ ‡è½¬åŒ–ä¸ºå†³ç­–", 121),
        ("super-power", "è¶…èƒ½åŠ›ï¼šé€Ÿåº¦ä¸æ•°æ®", 139),
        ("baseline-choice", "é€‰æ‹©ä½ çš„åŸºçº¿æ¨¡å‹", 169),
        ("training-framework", "é€‰æ‹©è®­ç»ƒæ¡†æ¶", 228),
        ("ablation-setup", "æ¶ˆèå®éªŒè®¾ç½®", 261),
        ("architecture-choices", "æ¶æ„é€‰æ‹©", 473),
        ("tokenizer", "åˆ†è¯å™¨", 1184),
        ("optimiser-hyperparameters", "ä¼˜åŒ–å™¨ä¸è®­ç»ƒè¶…å‚æ•°", 1430),
        ("scaling-laws", "æ‰©å±•å®šå¾‹", 1675),
        ("data-mixture", "æ•°æ®æ··åˆä¸ç­–åˆ’", 1707),
        ("pre-flight-checklist", "èµ·é£å‰æ£€æŸ¥æ¸…å•", 1821),
        ("scaling-surprises", "æ‰©å±•ä¸­çš„æ„å¤–", 1839),
        ("staying-course", "ä¿æŒèˆªå‘", 1987),
        ("mid-training", "ä¸­æœŸè®­ç»ƒ", 2039),
        ("wrapping-pretraining", "é¢„è®­ç»ƒæ”¶å°¾", 2081),
        ("post-training-compass", "åè®­ç»ƒæŒ‡å—é’ˆ", 2108),
        ("evals-first", "é¦–è¦ä¹‹äº‹ï¼šå…ˆæè¯„ä¼°", 2146),
        ("tools-trade", "è¡Œä¸šå·¥å…·", 2241),
        ("sft-start", "ä¸ºä½•æ‰€æœ‰åè®­ç»ƒæµç¨‹éƒ½ä»SFTå¼€å§‹", 2277),
        ("preference-optimization", "ä»SFTåˆ°åå¥½ä¼˜åŒ–", 2772),
        ("online-policy", "èµ°å‘åœ¨çº¿ç­–ç•¥å¹¶è¶…è¶Šç›‘ç£æ ‡ç­¾", 2880),
        ("wrapping-post-training", "æ”¶å°¾ï¼šåè®­ç»ƒé˜¶æ®µ", 3074),
        ("gpu-architecture", "GPUå†…éƒ¨ï¼šå†…éƒ¨æ¶æ„", 3097),
        ("gpu-communication", "GPUä¹‹å¤–ï¼šGPUå¦‚ä½•ä¸å¤–ç•Œé€šä¿¡", 3312),
        ("resilient-systems", "æ„å»ºå¼¹æ€§è®­ç»ƒç³»ç»Ÿ", 4051),
        ("optimizing-throughput", "ä¼˜åŒ–è®­ç»ƒååé‡", 4171),
        ("acknowledgments", "è‡´è°¢", 4323)
    ]

    # Initialize sections
    sections = []

    # Find the actual line numbers for each section
    section_boundaries = []
    for i, (section_id, title, expected_line) in enumerate(major_sections):
        # Search for the section title in the file
        found_line = None
        for line_num, line in enumerate(lines, 1):
            if title in line and line.startswith('#'):
                found_line = line_num
                break

        if found_line:
            section_boundaries.append((section_id, title, found_line))
        else:
            print(f"Warning: Could not find section '{title}'")

    # Sort by line number
    section_boundaries.sort(key=lambda x: x[2])

    # Create sections by extracting content between boundaries
    for i, (section_id, title, start_line) in enumerate(section_boundaries):
        end_line = section_boundaries[i + 1][2] if i + 1 < len(section_boundaries) else len(lines)

        # Extract content for this section
        section_lines = lines[start_line - 1:end_line - 1]  # Convert to 0-indexed

        # Remove leading empty lines
        while section_lines and not section_lines[0].strip():
            section_lines.pop(0)

        # Remove trailing empty lines
        while section_lines and not section_lines[-1].strip():
            section_lines.pop()

        section_content = '\n'.join(section_lines)

        if section_content.strip():
            sections.append({
                'id': section_id,
                'title': title,
                'content': section_content,
                'start_line': start_line,
                'end_line': end_line - 1
            })

    # Write sections to files
    for section in sections:
        filename = f"{section['id']}.md"
        filepath = os.path.join(output_dir, filename)

        # Add section title as main heading if not present
        content = section['content']
        if not content.startswith('#'):
            content = f"# {section['title']}\n\n{content}"

        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)

        print(f"Created: {filepath} (lines {section['start_line']}-{section['end_line']})")

    # Create an index file
    create_index_file(sections, output_dir)

    print(f"\nSplit {len(sections)} sections into '{output_dir}' directory")
    return sections

def create_index_file(sections, output_dir):
    """Create an index file with links to all sections."""

    index_content = "# Smol è®­ç»ƒæ‰‹å†Œ - ç« èŠ‚ç´¢å¼•\n\n"
    index_content += "æœ¬æ–‡ä»¶æ˜¯æ ¹æ®è¯­ä¹‰ä¿¡æ¯æ‹†åˆ†åçš„ç« èŠ‚ç´¢å¼•ã€‚\n\n"
    index_content += "## ç« èŠ‚åˆ—è¡¨\n\n"

    for section in sections:
        index_content += f"- [{section['title']}]({section['id']}.md) (ç¬¬ {section['start_line']}-{section['end_line']} è¡Œ)\n"

    index_filepath = os.path.join(output_dir, "README.md")
    with open(index_filepath, 'w', encoding='utf-8') as f:
        f.write(index_content)

    print(f"Created index: {index_filepath}")

def create_summary_sections(file_path, output_dir="sections"):
    """Create additional semantic groupings based on content analysis."""

    # Read the original file
    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Define thematic groupings
    thematic_sections = [
        {
            "id": "pretraining-complete",
            "title": "é¢„è®­ç»ƒå®Œæ•´æŒ‡å—",
            "description": "ä»é›¶å¼€å§‹é¢„è®­ç»ƒä¸€ä¸ªLLMçš„å®Œæ•´æµç¨‹",
            "keywords": ["é¢„è®­ç»ƒ", "æ•°æ®", "æ¶æ„", "è¶…å‚æ•°", "æ¶ˆèå®éªŒ"]
        },
        {
            "id": "post-training-complete",
            "title": "åè®­ç»ƒå®Œæ•´æŒ‡å—",
            "description": "SFTã€DPOã€RLHFç­‰åè®­ç»ƒæŠ€æœ¯",
            "keywords": ["åè®­ç»ƒ", "SFT", "DPO", "åå¥½ä¼˜åŒ–", "RL"]
        },
        {
            "id": "infrastructure-complete",
            "title": "åŸºç¡€è®¾æ–½å®Œæ•´æŒ‡å—",
            "description": "GPUé›†ç¾¤ã€ç½‘ç»œã€å­˜å‚¨å’Œæ€§èƒ½ä¼˜åŒ–",
            "keywords": ["GPU", "åŸºç¡€è®¾æ–½", "å¹¶è¡Œ", "ç½‘ç»œ", "å­˜å‚¨"]
        }
    ]

    # Create summary files for each thematic section
    for theme in thematic_sections:
        theme_content = f"# {theme['title']}\n\n"
        theme_content += f"{theme['description']}\n\n"
        theme_content += "## ç›¸å…³ç« èŠ‚\n\n"

        # This is a simplified approach - in a real implementation,
        # you would analyze content to match keywords
        if "pretraining" in theme['id']:
            theme_content += "- [è®­ç»ƒæŒ‡å—é’ˆ](training-compass.md)\n"
            theme_content += "- [æ¶æ„é€‰æ‹©](architecture-choices.md)\n"
            theme_content += "- [æ•°æ®æ··åˆä¸ç­–åˆ’](data-mixture.md)\n"
            theme_content += "- [æ‰©å±•å®šå¾‹](scaling-laws.md)\n"
        elif "post-training" in theme['id']:
            theme_content += "- [åè®­ç»ƒæŒ‡å—é’ˆ](post-training-compass.md)\n"
            theme_content += "- [ä¸ºä½•æ‰€æœ‰åè®­ç»ƒæµç¨‹éƒ½ä»SFTå¼€å§‹](sft-start.md)\n"
            theme_content += "- [ä»SFTåˆ°åå¥½ä¼˜åŒ–](preference-optimization.md)\n"
            theme_content += "- [èµ°å‘åœ¨çº¿ç­–ç•¥](online-policy.md)\n"
        elif "infrastructure" in theme['id']:
            theme_content += "- [GPUå†…éƒ¨æ¶æ„](gpu-architecture.md)\n"
            theme_content += "- [GPUé€šä¿¡](gpu-communication.md)\n"
            theme_content += "- [æ„å»ºå¼¹æ€§è®­ç»ƒç³»ç»Ÿ](resilient-systems.md)\n"
            theme_content += "- [ä¼˜åŒ–è®­ç»ƒååé‡](optimizing-throughput.md)\n"

        theme_filepath = os.path.join(output_dir, f"{theme['id']}.md")
        with open(theme_filepath, 'w', encoding='utf-8') as f:
            f.write(theme_content)

        print(f"Created thematic section: {theme_filepath}")

if __name__ == "__main__":
    input_file = "/Users/peyton/Workspace/smol_training/README_new.md"

    if not os.path.exists(input_file):
        print(f"Error: Input file '{input_file}' not found")
        exit(1)

    print("Splitting markdown file into semantic sections...")
    sections = split_markdown_into_sections(input_file)

    print("\nCreating thematic summaries...")
    create_summary_sections(input_file)

    print("\nDone! ğŸ‰")
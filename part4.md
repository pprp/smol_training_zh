èŠ‚ç‚¹é¢„ç•™ï¼ˆNode reservationï¼‰ï¼š ç”±äº SmolLM3 æ˜¯åœ¨ç”± Slurm ç®¡ç†çš„é›†ç¾¤ä¸Šè®­ç»ƒçš„ï¼Œæˆ‘ä»¬ä¸ºæ•´ä¸ªè®­ç»ƒè¿‡ç¨‹é¢„è®¢äº†å›ºå®šçš„ 48 ä¸ªèŠ‚ç‚¹ã€‚è¿™ç§è®¾ç½®è®©æˆ‘ä»¬èƒ½å¤ŸæŒç»­è¿½è¸ªåŒä¸€æ‰¹èŠ‚ç‚¹çš„å¥åº·ä¸æ€§èƒ½ï¼Œä¹Ÿè§£å†³äº†å‰æ–‡æåˆ°çš„æ•°æ®å­˜å‚¨é—®é¢˜ã€‚æˆ‘ä»¬è¿˜é¢„ç•™äº†ä¸€ä¸ªå¤‡ç”¨èŠ‚ç‚¹ï¼ˆå°±åƒæ±½è½¦çš„å¤‡èƒï¼‰ï¼Œä¸€æ—¦æŸä¸ªèŠ‚ç‚¹æ•…éšœï¼Œå¯ç«‹å³æ›¿æ¢ï¼Œæ— éœ€ç­‰å¾…ç»´ä¿®ã€‚ç©ºé—²æ—¶ï¼Œè¯¥å¤‡ç”¨èŠ‚ç‚¹ä¼šè¿è¡Œè¯„ä¼°ä»»åŠ¡æˆ–å¼€å‘å®éªŒã€‚

æŒç»­ç›‘æ§ï¼ˆContinuous monitoringï¼‰ï¼š è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬å®æ—¶è·Ÿè¸ªæ‰€æœ‰èŠ‚ç‚¹çš„å…³é”®æŒ‡æ ‡ï¼ŒåŒ…æ‹¬ GPU æ¸©åº¦ã€å†…å­˜ç”¨é‡ã€è®¡ç®—åˆ©ç”¨ç‡ä¸ååæ³¢åŠ¨ã€‚æˆ‘ä»¬ä½¿ç”¨ [Prometheus](https://prometheus.io/) æ”¶é›†æ‰€æœ‰ GPU çš„ [DCGM](https://github.com/NVIDIA/DCGM) æŒ‡æ ‡ï¼Œå¹¶åœ¨ [Grafana](https://grafana.com/) ä»ªè¡¨æ¿ä¸­å¯è§†åŒ–ï¼Œå®ç°å®æ—¶ç›‘æ§ã€‚å¦‚éœ€åœ¨ AWS åŸºç¡€è®¾æ–½ä¸Šéƒ¨ç½² Prometheus ä¸ Grafana è¿›è¡Œ GPU ç›‘æ§çš„è¯¦ç»†æ­¥éª¤ï¼Œè¯·å‚è€ƒ[æ­¤ç¤ºä¾‹è®¾ç½®æŒ‡å—](https://github.com/aws-samples/awsome-distributed-training/tree/3ae961d022399021cc4053c3ba19b182ca6b8dc8/4.validation_and_observability/4.prometheus-grafana)ã€‚Slack æœºå™¨äººä¼šåœ¨ä»»ä½•èŠ‚ç‚¹å‡ºç°å¼‚å¸¸è¡Œä¸ºæ—¶å‘å‡ºå‘Šè­¦ï¼Œä½¿æˆ‘ä»¬èƒ½åœ¨ç¡¬ä»¶å½»åº•å´©æºƒå‰ä¸»åŠ¨æ›´æ¢ã€‚

[è®¿é—®ä»ªè¡¨æ¿](https://huggingfacetb-smol-training-playbook.hf.space/screencapture-grafana-huggingface.pdf) è¿™ç§å¤šå±‚ç­–ç•¥è®©ç¡¬ä»¶é—®é¢˜å˜æˆäº†å¯æ§çš„ä¸­æ–­ã€‚

çƒ­ç°å®æ£€éªŒï¼šå½“ GPU é™é€Ÿæ—¶

è¥é”€è§„æ ¼å‡è®¾å®Œç¾æ•£çƒ­ï¼Œä½†ç°å®æ›´å¤æ‚ã€‚GPU åœ¨è¿‡çƒ­æ—¶ä¼šè‡ªåŠ¨é™ä½æ—¶é’Ÿé¢‘ç‡ï¼Œå³ä½¿ç³»ç»Ÿè®¾è®¡è‰¯å¥½ï¼Œæ€§èƒ½ä¹Ÿä¼šä½äºç†è®ºæœ€å¤§å€¼ã€‚

![å›¾ 21ï¼šå›¾ç‰‡](https://huggingfacetb-smol-training-playbook.hf.space/_astro/image_27d1384e-bcac-80b1-9ffb-ec29d0021ccc.D54wWyJ9_2jmnNO.webp)

è¿™ä¸ª Grafana é¢æ¿å±•ç¤ºäº†æˆ‘ä»¬æ•´ä¸ª GPU é›†ç¾¤çš„çƒ­èŠ‚æµï¼ˆthermal throttlingï¼‰äº‹ä»¶ã€‚åº•éƒ¨é¢æ¿ä¸­çš„æ¡å½¢å›¾è¡¨ç¤º GPU å› è¿‡çƒ­è€Œè‡ªåŠ¨é™ä½æ—¶é’Ÿé¢‘ç‡çš„æ—¶åˆ»ã€‚

æˆ‘ä»¬é€šè¿‡ç›‘æ§æ¥è‡ª [NVIDIA DCGM](https://github.com/NVIDIA/DCGM/tree/master) çš„æŒ‡æ ‡ `DCGM_FI_DEV_CLOCK_THROTTLE_REASONS` æ¥æ£€æµ‹çƒ­èŠ‚æµã€‚å½“è¯¥æŒ‡æ ‡å‡ºç°éé›¶å€¼æ—¶ï¼Œè¯´æ˜ GPU æ­£å› è¿‡çƒ­è€Œè‡ªåŠ¨é™é¢‘ã€‚ä¸Šé¢çš„é¢æ¿å±•ç¤ºäº†è¿™äº›èŠ‚æµäº‹ä»¶åœ¨å®é™…è¿è¡Œä¸­çš„è¡¨ç°ã€‚

çƒ­èŠ‚æµä¸ä»…ä¼šå½±å“å—å½±å“çš„ GPUï¼Œè¿˜ä¼šåœ¨æ•´ä¸ªåˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒä¸­äº§ç”Ÿè¿é”ååº”ã€‚åœ¨æˆ‘ä»¬çš„æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°å•ä¸ªå‘ç”ŸèŠ‚æµçš„èŠ‚ç‚¹ä¼šæ˜¾è‘—æ‹–æ…¢é›†ä½“é€šä¿¡ï¼ˆcollective communicationï¼‰æ€§èƒ½ã€‚

åœ¨æˆ‘ä»¬å‹åŠ›æµ‹è¯•è¿‡ç¨‹ä¸­ï¼Œè·¨èŠ‚ç‚¹çš„ AllReduce å¸¦å®½å‡ºç°ä¸‹é™ã€‚å½“èŠ‚ç‚¹æ•°è¶…è¿‡ 14 ä¸ªåï¼Œå¸¦å®½ä» 350 GB/s éª¤é™è‡³ 100 GB/sï¼Œå…¶æ ¹æœ¬åŸå› å°±æ˜¯ä¸€å° GPU å‘ç”Ÿäº†çƒ­èŠ‚æµï¼Œè¿™è¯´æ˜äº†å•ä¸ªæ…¢èŠ‚ç‚¹å°±èƒ½æˆä¸ºæ•´ä¸ªåˆ†å¸ƒå¼è®­ç»ƒç®¡é“çš„ç“¶é¢ˆã€‚

ä¸Šå›¾å±•ç¤ºäº†éšç€èŠ‚ç‚¹æ•°ä» 1 æ‰©å±•åˆ° 16ï¼ŒAllReduce å¸¦å®½çš„é€€åŒ–æƒ…å†µã€‚æ³¨æ„åœ¨ 14 ä¸ªèŠ‚ç‚¹ä¹‹åå‡ºç°çš„æ€¥å‰§ä¸‹é™ï¼šå¸¦å®½ä» 350 GB/s æ‰åˆ° 100 GB/sï¼Œè€Œæˆ‘ä»¬åŸæœ¬é¢„æœŸå¸¦å®½åº”ä¿æŒåœ¨ 300 GB/s ä»¥ä¸Šï¼ˆæ­¤å‰å·²è§‚æµ‹åˆ°ï¼‰ã€‚è¿™å¹¶ä¸æ˜¯ç½‘ç»œé—®é¢˜ï¼šå•ä¸ªå‘ç”Ÿçƒ­èŠ‚æµçš„èŠ‚ç‚¹æˆäº†ç“¶é¢ˆï¼Œåœ¨æ¢¯åº¦åŒæ­¥é˜¶æ®µè¿«ä½¿æ‰€æœ‰å…¶ä»–èŠ‚ç‚¹ç­‰å¾…ã€‚åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ•´ä½“é€Ÿåº¦å–å†³äºæœ€æ…¢çš„é‚£ä¸ªèŠ‚ç‚¹ã€‚

ğŸ‘‰ å…³é”®æ•™è®­ï¼š åœ¨å¯åŠ¨é•¿æ—¶é—´è®­ç»ƒä¹‹å‰ï¼ŒåŠ¡å¿…å…ˆç”¨å‰æ–‡æåˆ°çš„å·¥å…·å¯¹ç¡¬ä»¶è¿›è¡Œå‹åŠ›æµ‹è¯•ï¼Œä»¥å‘ç°æ•£çƒ­å’Œä¾›ç”µç“¶é¢ˆã€‚è®­ç»ƒè¿‡ç¨‹ä¸­åº”æŒç»­ä½¿ç”¨ DCGM é¥æµ‹ç›‘æ§æ¸©åº¦ï¼Œå¹¶ä¸ºå®é™…çš„çƒ­è®¾è®¡æé™åšå¥½é¢„æ¡ˆã€‚åŒæ—¶ï¼Œå»ºè®®ç¡®è®¤ GPU æ—¶é’Ÿå·²é”å®šåœ¨æœ€é«˜æ€§èƒ½æ¡£ä½ã€‚è‹¥æƒ³æ·±å…¥äº†è§£ä¸ºä½• GPU ä¼šå› åŠŸè€—é™åˆ¶è€Œæ— æ³•æŒç»­è¾¾åˆ°æ ‡ç§°æ€§èƒ½ï¼Œè¯·å‚é˜…è¿™ç¯‡å…³äºåŠŸè€—é™é¢‘ï¼ˆpower throttlingï¼‰çš„[ç²¾å½©åˆ†æ](https://www.thonking.ai/p/strangely-matrix-multiplications)ã€‚

#### [Checkpoint Managementï¼ˆæ£€æŸ¥ç‚¹ç®¡ç†ï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#checkpoint-management)

æ£€æŸ¥ç‚¹ï¼ˆcheckpointï¼‰æ˜¯æˆ‘ä»¬åœ¨é•¿æ—¶é—´è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®‰å…¨ç½‘ã€‚æˆ‘ä»¬å®šæœŸä¿å­˜å®ƒä»¬ï¼Œå‡ºäºä¸‰ä¸ªå®é™…åŸå› ï¼šä»æ•…éšœä¸­æ¢å¤ã€é€šè¿‡è¯„ä¼°ç›‘æ§è®­ç»ƒè¿›åº¦ï¼Œä»¥åŠä¸ç¤¾åŒºå…±äº«ä¸­é—´æ¨¡å‹ä»¥ä¾›ç ”ç©¶ã€‚æ¢å¤æ–¹é¢æœ€ä¸ºé‡è¦ã€‚å¦‚æœæˆ‘ä»¬çš„è¿è¡Œå¤±è´¥ï¼Œæˆ‘ä»¬å¸Œæœ›ä»æœ€æ–°ä¿å­˜çš„æ£€æŸ¥ç‚¹é‡æ–°å¯åŠ¨ï¼Œè¿™æ ·å¦‚æœæˆ‘ä»¬ç«‹å³æ¢å¤ï¼Œæœ€å¤šåªä¼šä¸¢å¤±ä¿å­˜é—´éš”çš„æ—¶é—´ï¼ˆä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æ¯ 4 å°æ—¶ä¿å­˜ä¸€æ¬¡ï¼Œåˆ™æœ€å¤šä¸¢å¤± 4 å°æ—¶çš„è®­ç»ƒï¼‰ã€‚

å°½é‡è‡ªåŠ¨åŒ–ä½ çš„æ¢å¤è¿‡ç¨‹ã€‚ä¾‹å¦‚ï¼Œåœ¨ Slurm ä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨ `SBATCH --requeue`ï¼Œè¿™æ ·ä½œä¸šä¼šä»æœ€æ–°çš„æ£€æŸ¥ç‚¹è‡ªåŠ¨é‡å¯ã€‚è¿™æ ·ï¼Œä½ å¯ä»¥é¿å…æµªè´¹æ—¶é—´ç­‰å¾…æœ‰äººæ³¨æ„åˆ°æ•…éšœå¹¶æ‰‹åŠ¨é‡å¯ã€‚

åœ¨å®ç°æ¢å¤æœºåˆ¶æ—¶ï¼Œæœ‰ä¸¤ä¸ªé‡è¦ç»†èŠ‚éœ€è¦ç‰¢è®°ï¼š

*   æ£€æŸ¥ç‚¹ä¿å­˜åº”åœ¨åå°è¿›è¡Œï¼Œä¸å½±å“è®­ç»ƒååé‡ï¼ˆthroughputï¼‰ã€‚
*   æ³¨æ„ä½ çš„å­˜å‚¨ç©ºé—´ï¼Œåœ¨ä¸€ä¸ª 24 å¤©çš„è¿è¡Œä¸­ï¼Œæ¯ 4 å°æ—¶ä¿å­˜ä¸€æ¬¡æ„å‘³ç€å¤§çº¦ 144 ä¸ªæ£€æŸ¥ç‚¹ã€‚å¯¹äºå¤§å‹æ¨¡å‹å’Œä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆoptimizer statesï¼‰ï¼Œè¿™ä¼šè¿…é€Ÿç´¯ç§¯ã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæˆ‘ä»¬ä¸€æ¬¡åªå­˜å‚¨ä¸€ä¸ªæœ¬åœ°æ£€æŸ¥ç‚¹ï¼ˆæœ€æ–°ä¿å­˜çš„ï¼‰ï¼Œå…¶ä½™çš„å¸è½½åˆ° S3ï¼Œä»¥é¿å…å¡«æ»¡é›†ç¾¤å­˜å‚¨ã€‚

è¿‡å»çš„ä¸€ä¸ªæƒ¨ç—›æ•™è®­ï¼š

åœ¨æˆ‘ä»¬ç¬¬ä¸€æ¬¡å¤§è§„æ¨¡è¿è¡Œï¼ˆStarCoder 15Bï¼‰æœŸé—´ï¼Œè®­ç»ƒåœ¨å¤šæ¬¡é‡å¯ä¸­é¡ºåˆ©è¿›è¡Œã€‚åœ¨æœ€åä¸€å¤©ï¼Œæˆ‘ä»¬å‘ç°æ•´ä¸ªæ£€æŸ¥ç‚¹æ–‡ä»¶å¤¹è¢«è„šæœ¬æœ«å°¾é—ç•™çš„ `rm -rf $CHECKPOINT_PATH` å‘½ä»¤åˆ é™¤äº†ï¼Œè¿™ä¸ªå‘½ä»¤æ¥è‡ªæ—§çš„ååé‡æµ‹è¯•ã€‚è¿™ä¸ªç ´åæ€§å‘½ä»¤åªæœ‰åœ¨ Slurm ä½œä¸šçœŸæ­£å®Œæˆæ—¶æ‰ä¼šè§¦å‘ï¼Œè€Œä¹‹å‰çš„é‡å¯ä¸­ä½œä¸šä»æœªçœŸæ­£å®Œæˆè¿‡ã€‚

å¹¸è¿çš„æ˜¯ï¼Œæˆ‘ä»¬ä¿å­˜äº†å‰ä¸€å¤©çš„ checkpointï¼ˆæ£€æŸ¥ç‚¹ï¼‰ï¼Œå› æ­¤åªæŸå¤±äº†ä¸€å¤©çš„é‡è®­æ—¶é—´ã€‚æ•™è®­å¾ˆæ˜ç¡®ï¼šæ°¸è¿œä¸è¦æŠŠç ´åæ€§å‘½ä»¤ç•™åœ¨ç”Ÿäº§è„šæœ¬ä¸­ï¼Œå¹¶ä¸”åœ¨ä¿å­˜åç«‹å³è‡ªåŠ¨åŒ– checkpoint å¤‡ä»½ï¼Œè€Œä¸æ˜¯ä¾èµ–äººå·¥å¹²é¢„ã€‚

åœ¨æˆ‘ä»¬çš„ nanotron è®­ç»ƒä¸­ï¼Œæˆ‘ä»¬æ¯ 2 å°æ—¶åœ¨æœ¬åœ°ä¿å­˜ä¸€æ¬¡ checkpointï¼Œéšåç«‹å³å°†å…¶ä¸Šä¼ åˆ° S3ï¼Œä¸€æ—¦å¤‡ä»½ç¡®è®¤å°±åˆ é™¤æœ¬åœ°å‰¯æœ¬ã€‚æ¢å¤æ—¶ï¼Œå¦‚æœæœ€æ–°çš„ checkpoint åœ¨æœ¬åœ°ä¸å¯ç”¨ï¼Œå°±ä» S3 æ‹‰å–ã€‚è¿™ç§æ–¹æ³•æ—¢èŠ‚çœå­˜å‚¨ï¼Œåˆç¡®ä¿å¤‡ä»½ï¼Œè¿˜èƒ½å®ç°å¿«é€Ÿæ¢å¤ã€‚

#### [è‡ªåŠ¨åŒ–è¯„ä¼°](https://huggingfacetb-smol-training-playbook.hf.space/#automated-evaluations)

æ‰‹åŠ¨è¿è¡Œè¯„ä¼°å¾ˆå¿«å°±ä¼šæˆä¸ºç“¶é¢ˆã€‚çœ‹èµ·æ¥ç®€å•ï¼Œä½†ä¸€æ—¦éœ€è¦åå¤æ‰§è¡Œï¼Œè·‘åŸºå‡†ã€è¿½è¸ªå¹¶ç»˜åˆ¶æ¯æ¬¡å®éªŒçš„ç»“æœï¼Œå¼€é”€å°±ä¼šè¿…é€Ÿç´¯ç§¯ã€‚è§£å†³ä¹‹é“ï¼Ÿä¸€å¼€å§‹å°±å…¨éƒ¨è‡ªåŠ¨åŒ–ã€‚

å¯¹äº SmolLM3ï¼Œæˆ‘ä»¬ä½¿ç”¨ [LightEval](https://github.com/huggingface/lighteval) åœ¨ nanotron æ£€æŸ¥ç‚¹ä¸Šè¿è¡Œè¯„ä¼°ã€‚æ¯ä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹ï¼Œé›†ç¾¤å°±ä¼šè‡ªåŠ¨è§¦å‘ä¸€æ¬¡è¯„ä¼°ä»»åŠ¡ã€‚ç»“æœç›´æ¥æ¨é€åˆ° Weights & Biases æˆ– [Trackio](https://github.com/gradio-app/trackio)ï¼Œæˆ‘ä»¬åªéœ€æ‰“å¼€ä»ªè¡¨æ¿ï¼Œå°±èƒ½çœ‹åˆ°æ›²çº¿å®æ—¶å˜åŒ–ã€‚è¿™ä¸ºæˆ‘ä»¬èŠ‚çœäº†å¤§é‡æ—¶é—´ï¼Œå¹¶ç¡®ä¿æ•´ä¸ªè®­ç»ƒè¿‡ç¨‹ä¸­è¯„ä¼°è¿½è¸ªçš„ä¸€è‡´æ€§ã€‚

å¦‚æœä½ çš„è®­ç»ƒæµç¨‹åªèƒ½è‡ªåŠ¨åŒ–ä¸€ä»¶äº‹ï¼Œé‚£å°±æŠŠè¯„ä¼°è‡ªåŠ¨åŒ–ã€‚

æœ€åï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•ä¼˜åŒ–è®­ç»ƒå¸ƒå±€ï¼ˆtraining layoutï¼‰ï¼Œä¹Ÿå°±æ˜¯æ¨¡å‹åœ¨å¯ç”¨ GPU ä¸Šçš„åˆ†å¸ƒæ–¹å¼ï¼Œä»¥æœ€å¤§åŒ–ååé‡ã€‚

### [ä¼˜åŒ–è®­ç»ƒååé‡](https://huggingfacetb-smol-training-playbook.hf.space/#optimizing-training-throughput)

#### [æˆ‘ä»¬éœ€è¦å¤šå°‘å— GPUï¼Ÿ](https://huggingfacetb-smol-training-playbook.hf.space/#how-many-gpus-do-we-need)

å¥½é—®é¢˜ï¼èŠäº†è¿™ä¹ˆå¤šè§„æ ¼å’ŒåŸºå‡†ï¼Œä½ è¿˜å¾—è§£å†³ä¸€ä¸ªå®é™…é—®é¢˜ï¼šåˆ°åº•è¯¥ç§Ÿæˆ–ä¹°å¤šå°‘å— GPUï¼Ÿ

ç¡®å®šåˆé€‚çš„ GPU æ•°é‡éœ€è¦åœ¨è®­ç»ƒæ—¶é—´ã€æˆæœ¬å’Œæ‰©å±•æ•ˆç‡ä¹‹é—´å–å¾—å¹³è¡¡ã€‚ä»¥ä¸‹æ˜¯æˆ‘ä»¬é‡‡ç”¨çš„æ¡†æ¶ï¼š

åŸºç¡€è§„æ¨¡ä¼°ç®—å…¬å¼ï¼š

GPU æ•°é‡ = æ€» FLOPs éœ€æ±‚ / (å• GPU ååé‡ Ã— ç›®æ ‡è®­ç»ƒæ—¶é—´)

è¿™ä¸ªå…¬å¼æŠŠé—®é¢˜æ‹†æˆä¸‰ä¸ªå…³é”®éƒ¨åˆ†ï¼š

*   æ€» FLOPs éœ€æ±‚ï¼ˆTotal FLOPs Requiredï¼‰ï¼šè®­ç»ƒæ¨¡å‹æ‰€éœ€çš„è®¡ç®—é‡ï¼ˆå–å†³äºæ¨¡å‹å¤§å°ã€è®­ç»ƒ token æ•°å’Œæ¶æ„ï¼‰
*   å• GPU ååé‡ï¼ˆPer-GPU Throughputï¼‰ï¼šæ¯å— GPU å®é™…èƒ½æä¾›çš„ FLOPs/sï¼ˆä¸æ˜¯ç†è®ºå³°å€¼ï¼ï¼‰
*   ç›®æ ‡è®­ç»ƒæ—¶é—´ï¼ˆTarget Training Timeï¼‰ï¼šä½ æ„¿æ„ç­‰å¾…è®­ç»ƒå®Œæˆçš„æ—¶é—´

å…³é”®æ´å¯Ÿï¼šä½ éœ€è¦ä¼°ç®—å®é™…ååé‡ï¼ˆrealistic throughputï¼‰ï¼Œè€Œéå³°å€¼è§„æ ¼ã€‚è¿™æ„å‘³ç€è¦è€ƒè™‘ Model FLOPs Utilizationï¼ˆMFUï¼Œæ¨¡å‹ FLOPs åˆ©ç”¨ç‡ï¼‰ï¼šä½ åœ¨å®è·µä¸­èƒ½è¾¾åˆ°çš„ç†è®ºå³°å€¼æ€§èƒ½ç™¾åˆ†æ¯”ã€‚

å¯¹äº SmolLM3ï¼Œæˆ‘ä»¬çš„è®¡ç®—å¦‚ä¸‹ï¼š

*   æ¨¡å‹å¤§å°ï¼š30 äº¿å‚æ•°ï¼ˆ3B parametersï¼‰
*   è®­ç»ƒ token æ•°ï¼š11 ä¸‡äº¿ token
*   ç›®æ ‡è®­ç»ƒæ—¶é—´ï¼šçº¦ 4 å‘¨
*   é¢„æœŸ MFUï¼š30%ï¼ˆåŸºäºåŒè§„æ¨¡å®éªŒï¼‰

é¦–å…ˆï¼Œç”¨æ ‡å‡† transformer è¿‘ä¼¼â€”â€”æ¯ token 6N FLOPsï¼ˆN = å‚æ•°æ•°ï¼‰â€”â€”ä¼°ç®—æ€» FLOPs éœ€æ±‚ï¼š

æ€» FLOPs = 6 Ã— 3Ã—10â¹ å‚æ•° Ã— 11Ã—10Â¹Â² token = 1.98Ã—10Â²Â³ FLOPs

åœ¨ 30% çš„é¢„æœŸ MFU ä¸‹ï¼Œæ¯å— GPU çš„æœ‰æ•ˆååé‡å˜ä¸ºï¼š

æœ‰æ•ˆååé‡ = 720Ã—10Â¹Â² FLOPs/sec Ã— 0.30 = 216Ã—10Â¹Â² FLOPs/sec  
$$\text{Effective Throughput} = 720 \times 10^{12} \text{ FLOPs/sec} \times 0.30 = 216 \times 10^{12} \text{ FLOPs/sec}$$

ç°åœ¨ä»£å…¥æˆ‘ä»¬çš„è§„æ¨¡ä¼°ç®—å…¬å¼ï¼š

GPU æ•°é‡ = 1.98Ã—10Â²Â³ FLOPs / (216Ã—10Â¹Â² FLOPs/sec Ã— 4 weeks Ã— 604,800 sec/week)  
= 1.98Ã—10Â²Â³ / 5.23Ã—10Â²â° â‰ˆ 379 GPUs  
$$\text{GPU Count} = \frac{1.98 \times 10^{23} \text{ FLOPs}}{216 \times 10^{12} \text{ FLOPs/sec} \times 4 \text{ weeks} \times 604,800 \text{ sec/week}}  
= \frac{1.98 \times 10^{23}}{5.23 \times 10^{20}} \approx 379 \text{ GPUs}$$

è¿™ä¸€è®¡ç®—æŒ‡å‘ 375â€“400 å¼  H100ï¼Œæˆ‘ä»¬æœ€ç»ˆæ‹¿åˆ°äº† 384 å¼  H100ï¼Œè¿™ä¸ªæ•°å­—ä¸æˆ‘ä»¬çš„å¹¶è¡Œç­–ç•¥éå¸¸å¥‘åˆï¼Œå¹¶ç»™å‡ºäº†ä¸€ä¸ªç°å®å¯è¡Œçš„ 4 å‘¨æ—¶é—´è¡¨ï¼ŒåŒæ—¶ä¸ºèŠ‚ç‚¹æ•…éšœå’Œé‡å¯ç­‰æ„å¤–æƒ…å†µç•™å‡ºäº†ç¼“å†²ã€‚

---

ä¸ºä»€ä¹ˆæ›´å¤š GPU å¹¶ä¸æ€»æ˜¯æ›´å¥½ï¼šé˜¿å§†è¾¾å°”å®šå¾‹ï¼ˆAmdahlâ€™s Lawï¼‰åœ¨èµ·ä½œç”¨

è¿™é‡Œæœ‰ä¸€ä¸ªåç›´è§‰çš„äº‹å®ï¼šå¢åŠ  GPU å®é™…ä¸Šå¯èƒ½è®©ä½ çš„è®­ç»ƒå˜æ…¢ã€‚è¿™å°±æ˜¯ [é˜¿å§†è¾¾å°”å®šå¾‹](https://en.wikipedia.org/wiki/Amdahl%27s_law) ç™»åœºçš„åœ°æ–¹ã€‚

é˜¿å§†è¾¾å°”å®šå¾‹æŒ‡å‡ºï¼Œå¹¶è¡ŒåŒ–å¸¦æ¥çš„åŠ é€Ÿä»æ ¹æœ¬ä¸Šå—é™äºå·¥ä½œè´Ÿè½½ä¸­ä¸²è¡Œï¼ˆä¸å¯å¹¶è¡Œï¼‰éƒ¨åˆ†çš„æ¯”ä¾‹ã€‚åœ¨ LLM è®­ç»ƒä¸­ï¼Œè¿™éƒ¨åˆ†â€œä¸²è¡Œâ€ä¸»è¦æ˜¯é€šä¿¡å¼€é”€ï¼šåœ¨ GPU ä¹‹é—´åŒæ­¥æ¢¯åº¦/æƒé‡/æ¿€æ´»æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œè¿™éƒ¨åˆ†æ— æ³•é€šè¿‡å¹¶è¡ŒåŒ–æ¶ˆé™¤ï¼ˆæ›´å¤šé˜…è¯»è§[æ­¤å¤„](https://acenet-arc.github.io/ACENET_Summer_School_General/05-performance/index.html)ï¼‰ã€‚

å…¬å¼ä¸ºï¼š 

$$\text{æœ€å¤§åŠ é€Ÿæ¯”} = \frac{1}{\text{ä¸²è¡Œæ¯”ä¾‹} + \frac{\text{å¹¶è¡Œæ¯”ä¾‹}}{\text{å¤„ç†å™¨æ•°é‡}}}$$

å¯¹äº SmolLM3 çš„ 3B æ¨¡å‹ï¼Œå¦‚æœé€šä¿¡å ç”¨æ¯ä¸€æ­¥ 10 % çš„æ—¶é—´ï¼Œé‚£ä¹ˆæ— è®ºä½ å¢åŠ å¤šå°‘ GPUï¼Œéƒ½æ— æ³•è·å¾—è¶…è¿‡ 10 å€çš„åŠ é€Ÿã€‚æ›´ç³Ÿçš„æ˜¯ï¼Œéšç€ GPU æ•°é‡å¢åŠ ï¼Œé€šä¿¡å æ¯”å¾€å¾€è¿˜ä¼šä¸Šå‡ï¼Œå› ä¸ºï¼š

* æ›´å¤š GPU = æ›´å¤š AllReduce å‚ä¸è€… = æ›´é•¿çš„åŒæ­¥æ—¶é—´  
* ç½‘ç»œå»¶è¿Ÿ/å¸¦å®½æˆä¸ºç“¶é¢ˆ  
* å°æ¨¡å‹æ— æ³•æŠŠé€šä¿¡éšè—åœ¨è®¡ç®—èƒŒå

å¯¹äº SmolLM3ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å¼±æ‰©å±•ï¼ˆweak scalingï¼‰åŸåˆ™ï¼šå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆglobal batch sizeï¼‰éš GPU æ•°é‡çº¿æ€§æ‰©å±•ï¼Œä¿æŒæ¯å— GPU çº¦ 8K ä¸ª tokenã€‚è¿™æ ·æ—¢èƒ½ç»´æŒé€šä¿¡ä¸è®¡ç®—çš„åˆç†æ¯”ä¾‹ï¼Œåˆèƒ½æœ€å¤§åŒ–ååé‡ã€‚

#### [å¯»æ‰¾æœ€ä¼˜å¹¶è¡Œé…ç½®](https://huggingfacetb-smol-training-playbook.hf.space/#finding-the-optimal-parallelism-configuration)

ä¸€æ—¦ä½ æå®šäº† GPUï¼Œä¸‹ä¸€ä¸ªæŒ‘æˆ˜å°±æ˜¯æŠŠå®ƒä»¬é…ç½®å¾—èƒ½çœŸæ­£é«˜æ•ˆè®­ç»ƒã€‚æ­¤æ—¶ï¼Œå¹¶è¡Œç­–ç•¥ï¼ˆparallelism strategyï¼‰å°±æˆäº†å…³é”®ã€‚

æˆ‘ä»¬å€Ÿé‰´äº† [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=step_1%3A_fitting_a_training_step_in_memory)[å¯»æ‰¾æœ€ä¼˜è®­ç»ƒé…ç½®çš„æ–¹æ³•](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=step_1%3A_fitting_a_training_step_in_memory)ã€‚è¯¥æ‰‹å†ŒæŠŠé—®é¢˜æ‹†æˆä¸‰æ­¥ï¼šå…ˆç¡®ä¿æ¨¡å‹èƒ½æ”¾è¿›å†…å­˜ï¼Œå†è¾¾åˆ°ç›®æ ‡æ‰¹æ¬¡å¤§å°ï¼ˆbatch sizeï¼‰ï¼Œæœ€åæœ€å¤§åŒ–ååã€‚ä¸‹é¢çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•æŠŠè¿™å¥—æµç¨‹ç”¨åœ¨ SmolLM3 ä¸Šã€‚

#### [ç¬¬ 1 æ­¥ï¼šè®©è®­ç»ƒæ­¥è£…è¿›å†…å­˜](https://huggingfacetb-smol-training-playbook.hf.space/#step-1-fitting-a-training-step-in-memory)

ç¬¬ä¸€ä¸ªé—®é¢˜å¾ˆç®€å•ï¼šæˆ‘ä»¬çš„ SmolLM3 3B æ¨¡å‹èƒ½ä¸èƒ½å¡è¿›å•å¼  H100 çš„ 80 GB å†…å­˜ï¼Ÿä¸ºäº†å›ç­”å®ƒï¼Œæˆ‘ä»¬ä½¿ç”¨ [nanotron çš„](https://huggingface.co/spaces/nanotron/predict_memory)[`predict_memory`](https://huggingface.co/spaces/nanotron/predict_memory)[å·¥å…·](https://huggingface.co/spaces/nanotron/predict_memory)ï¼Œå®ƒèƒ½ä¼°ç®—æ¨¡å‹å‚æ•°ã€ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆoptimizer statesï¼‰ã€æ¢¯åº¦ï¼ˆgradientsï¼‰å’Œæ¿€æ´»å€¼ï¼ˆactivationsï¼‰çš„å†…å­˜å ç”¨ã€‚

ç»“æœæ˜¾ç¤ºæˆ‘ä»¬å·²é€¼è¿‘ 80 GB ä¸Šé™ã€‚è¿™æ„å‘³ç€å¿…é¡»é‡‡ç”¨æŸç§å¹¶è¡Œæ–¹å¼ï¼Œé™ä½æ¯å¼  GPU çš„å†…å­˜å ç”¨â€”â€”æ— è®ºæ˜¯ Tensor Parallelismï¼ˆå¼ é‡å¹¶è¡Œï¼ŒæŠŠæ¨¡å‹å±‚æ‹†åˆ°å¤šå¼  GPUï¼‰ã€Pipeline Parallelismï¼ˆæµæ°´çº¿å¹¶è¡Œï¼ŒæŠŠæ¨¡å‹æ·±åº¦æ‹†åˆ°å¤šå¼  GPUï¼‰ï¼Œè¿˜æ˜¯ ZeRO ä¼˜åŒ–å™¨åˆ†ç‰‡ï¼ˆZeRO optimizer shardingï¼ŒæŠŠä¼˜åŒ–å™¨çŠ¶æ€åˆ†å¸ƒå‡ºå»ï¼‰ã€‚å¦‚æœä¸ç”¨å…¶ä¸­è‡³å°‘ä¸€ç§ç­–ç•¥ï¼Œæˆ‘ä»¬å°±æ— æ³•é«˜æ•ˆè®­ç»ƒï¼Œç”šè‡³æ ¹æœ¬æ— æ³•è®­ç»ƒã€‚

#### [æ­¥éª¤ 2ï¼šè¾¾åˆ°ç›®æ ‡å…¨å±€æ‰¹æ¬¡å¤§å°](https://huggingfacetb-smol-training-playbook.hf.space/#step-2-achieving-the-target-global-batch-size)

æ—¢ç„¶æˆ‘ä»¬å·²ç»é€šè¿‡æŸç§å½¢å¼çš„å¹¶è¡Œï¼ˆparallelismï¼‰ç¡®è®¤æ¨¡å‹å¯ä»¥æ”¾è¿›æ˜¾å­˜ï¼Œæ¥ä¸‹æ¥å°±è¦ç¡®å®šå¦‚ä½•æŠŠå…¨å±€æ‰¹æ¬¡å¤§å°ï¼ˆGlobal Batch Sizeï¼ŒGBSï¼‰åšåˆ°çº¦ 200 ä¸‡ä¸ª tokenã€‚è¿™ä¸€çº¦æŸç»™å‡ºäº†ç¬¬ä¸€ä¸ªç­‰å¼ï¼š

$$\text{GBS} = \text{DP} \times \text{MBS} \times \text{GRAD\_ACC} \times \text{SEQLEN} \approx 2\text{M tokens}$$

å…¶ä¸­ï¼š

*   DPï¼ˆData Parallelismï¼Œæ•°æ®å¹¶è¡Œï¼‰ï¼šæ•°æ®å¹¶è¡Œå‰¯æœ¬çš„æ•°é‡  
*   MBSï¼ˆMicro Batch Sizeï¼Œå¾®æ‰¹æ¬¡å¤§å°ï¼‰ï¼šæ¯ä¸ª GPU åœ¨æ¯ä¸ªå¾®æ‰¹æ¬¡ä¸­å¤„ç†çš„ token æ•°  
*   GRAD_ACCï¼ˆGradient Accumulationï¼Œæ¢¯åº¦ç´¯ç§¯ï¼‰ï¼šåœ¨ä¼˜åŒ–å™¨æ›´æ–°å‰æ‰§è¡Œçš„ forward-backward æ¬¡æ•°  
*   SEQLENï¼ˆSequence Lengthï¼Œåºåˆ—é•¿åº¦ï¼‰ï¼šæ¯æ¡åºåˆ—çš„ token æ•°ï¼ˆç¬¬ä¸€é˜¶æ®µé¢„è®­ç»ƒä¸º 4096ï¼‰

æˆ‘ä»¬è¿˜å—åˆ° 384 å¼  H100 çš„ç¡¬ä»¶çº¦æŸï¼š

$$\text{DP} \times \text{TP} \times \text{PP} = 384 = 2^7 \times 3$$

å…¶ä¸­ï¼š

*   TPï¼ˆTensor Parallelismï¼Œå¼ é‡å¹¶è¡Œï¼‰ï¼šæ¯ä¸ªæ¨¡å‹å±‚æ‰€ç”¨çš„ GPU æ•°ï¼ˆæ‹†åˆ†æƒé‡çŸ©é˜µï¼‰  
*   PPï¼ˆPipeline Parallelismï¼Œæµæ°´çº¿å¹¶è¡Œï¼‰ï¼šæ¨¡å‹æ·±åº¦æ–¹å‘ä¸Šçš„ GPU æ•°ï¼ˆçºµå‘æ‹†åˆ†å±‚ï¼‰

è¿™ä¸¤ä¸ªç­‰å¼å…±åŒå®šä¹‰äº†æˆ‘ä»¬çš„æœç´¢ç©ºé—´ã€‚æˆ‘ä»¬éœ€è¦åœ¨æ»¡è¶³åŒé‡çº¦æŸçš„åŒæ—¶ï¼Œæ‰¾åˆ°èƒ½æœ€å¤§åŒ–è®­ç»ƒååé‡çš„å–å€¼ã€‚

#### [æ­¥éª¤ 3ï¼šä¼˜åŒ–è®­ç»ƒååé‡](https://huggingfacetb-smol-training-playbook.hf.space/#step-3-optimizing-training-throughput)

åœ¨ç¡®å®šäº†çº¦æŸæ¡ä»¶åï¼Œæˆ‘ä»¬éœ€è¦æ‰¾åˆ°èƒ½å¤Ÿæœ€å¤§åŒ–è®­ç»ƒååé‡çš„å¹¶è¡Œé…ç½®ã€‚æœç´¢ç©ºé—´ç”±æˆ‘ä»¬çš„ç¡¬ä»¶æ‹“æ‰‘å’Œæ¨¡å‹æ¶æ„å…±åŒå®šä¹‰ã€‚

å¦‚ä¸Šä¸€èŠ‚æ‰€è¿°ï¼Œæˆ‘ä»¬çš„ç¡¬ä»¶ç¯å¢ƒæä¾›ä¸¤ç§æˆªç„¶ä¸åŒçš„äº’è¿æ–¹å¼ï¼šç”¨äºèŠ‚ç‚¹å†…é€šä¿¡çš„ NVLinkï¼ˆ900 GB/sï¼‰å’Œç”¨äºèŠ‚ç‚¹é—´é€šä¿¡çš„ EFAï¼ˆ~50 GB/sï¼‰ã€‚è¿™ç§æ‹“æ‰‘å¤©ç„¶æç¤ºæˆ‘ä»¬è‡³å°‘åº”é‡‡ç”¨ä¸¤ç§å¹¶è¡Œå½¢å¼ï¼Œä»¥åŒ¹é…ç½‘ç»œç‰¹æ€§ã€‚è¿™ä¸¤ç§äº’è¿å¸¦å®½çš„å·¨å¤§å·®å¼‚å°†æå¤§å½±å“å“ªäº›å¹¶è¡Œç­–ç•¥è¡¨ç°æœ€ä½³ã€‚

ä»æ¨¡å‹è§’åº¦çœ‹ï¼ŒSmolLM3 çš„æ¶æ„é™åˆ¶äº†å¯é€‰æ–¹æ¡ˆã€‚ç”±äºæˆ‘ä»¬æœªé‡‡ç”¨æ··åˆä¸“å®¶ï¼ˆMixture-of-Expertsï¼ŒMoEï¼‰æ¶æ„ï¼Œå› æ­¤æ— éœ€ä¸“å®¶å¹¶è¡Œï¼ˆExpert Parallelismï¼‰ã€‚åŒæ ·ï¼Œç¬¬ä¸€é˜¶æ®µä»¥ 4096 çš„åºåˆ—é•¿åº¦è®­ç»ƒï¼Œä¹Ÿæ„å‘³ç€æ— éœ€ä¸Šä¸‹æ–‡å¹¶è¡Œï¼ˆContext Parallelismï¼‰ã€‚è¿™ç»™æˆ‘ä»¬ç•™ä¸‹äº†ä¸‰ä¸ªä¸»è¦å¹¶è¡Œç»´åº¦å¯ä¾›æ¢ç´¢ï¼šæ•°æ®å¹¶è¡Œï¼ˆData Parallelismï¼ŒDPï¼‰ã€å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼ŒTPï¼‰å’Œæµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼ŒPPï¼‰ã€‚

é‰´äºæ­¥éª¤ 2 çš„çº¦æŸï¼Œæˆ‘ä»¬éœ€è¦åœ¨ä»¥ä¸‹å‚æ•°èŒƒå›´å†…è¿›è¡Œæœç´¢ï¼š

*   å¸¦ ZeRO å˜ä½“çš„ DPï¼ˆZeRO-0ã€ZeRO-1ã€ZeRO-3ï¼‰ï¼šå–å€¼ 1 åˆ° 384ï¼Œä¸”éœ€ä¸º 2 å’Œ/æˆ– 3 çš„å€æ•°  
*   TPï¼ˆ1ã€2ã€3ã€4ã€6ã€8ï¼‰ï¼šé™åˆ¶åœ¨å•èŠ‚ç‚¹å†…ï¼Œä»¥å……åˆ†åˆ©ç”¨ NVLink çš„é«˜å¸¦å®½  
*   PPï¼ˆ1..48ï¼‰ï¼šå°†æ¨¡å‹æ·±åº¦æ‹†åˆ†åˆ°å¤šå¼  GPU  
*   MBSï¼ˆ2ã€3ã€4ã€5ï¼‰ï¼šæ ¹æ®å¹¶è¡Œå¸¦æ¥çš„å†…å­˜èŠ‚çœï¼Œå¯å¢å¤§ MBS ä»¥æ›´å¥½åœ°åˆ©ç”¨ Tensor Core  
*   æ¿€æ´»æ£€æŸ¥ç‚¹ï¼ˆActivation checkpointingï¼‰ï¼ˆæ— ã€é€‰æ‹©æ€§ã€å®Œæ•´ï¼‰ï¼šç”¨é¢å¤–è®¡ç®—æ¢å–å†…å­˜ä¸é€šä¿¡çš„å‡å°‘  
*   å†…æ ¸ä¼˜åŒ–ï¼ˆKernel optimizationsï¼‰ï¼šåœ¨å¯ç”¨å¤„å¯ç”¨ CUDA Graph ä¸ä¼˜åŒ–å†…æ ¸

å°½ç®¡ç»„åˆæ•°é‡çœ‹ä¼¼åºå¤§ï¼Œä¸€ä¸ªå®ç”¨çš„åšæ³•æ˜¯å…ˆç‹¬ç«‹æµ‹è¯•æ¯ä¸ªç»´åº¦ï¼Œç„¶åå‰”é™¤é‚£äº›æ˜æ˜¾æ‹–æ…¢ååé‡çš„é…ç½®ã€‚å…³é”®æ´è§åœ¨äºï¼šå¹¶éæ‰€æœ‰å¹¶è¡Œç­–ç•¥éƒ½ç”Ÿè€Œå¹³ç­‰ã€‚æœ‰äº›ç­–ç•¥å¼•å…¥çš„é€šä¿¡å¼€é”€è¿œè¶…å…¶æ”¶ç›Šï¼Œå°¤å…¶åœ¨æˆ‘ä»¬è¿™ç§è§„æ¨¡ä¸‹ã€‚

åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ï¼ŒPipeline Parallelismï¼ˆæµæ°´çº¿å¹¶è¡Œï¼ŒPPï¼‰ è¡¨ç°å‡ºè¾ƒå·®çš„æ€§èƒ½ç‰¹å¾ã€‚PP éœ€è¦åœ¨èŠ‚ç‚¹é—´é¢‘ç¹è¿›è¡Œ pipeline bubbleï¼ˆæµæ°´çº¿æ°”æ³¡ï¼‰åŒæ­¥ï¼Œè€Œå¯¹æˆ‘ä»¬ä»…æœ‰ 3B å‚æ•°çš„è¾ƒå°æ¨¡å‹è€Œè¨€ï¼Œé€šä¿¡å¼€é”€ç›–è¿‡äº†ä»»ä½•æ½œåœ¨æ”¶ç›Šã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿæ²¡æœ‰æ‹¿åˆ°èƒ½å½»åº•æ¶ˆé™¤æµæ°´çº¿æ°”æ³¡çš„é«˜æ•ˆ PP è°ƒåº¦æ–¹æ¡ˆï¼Œè¿™è¿›ä¸€æ­¥å‰Šå¼±äº† PP çš„å¯è¡Œæ€§ã€‚åŒæ ·ï¼ŒZeRO ç­‰çº§é«˜äº 0 æ—¶ä¼šå¼•å…¥å¤§é‡ all-gather ä¸ reduce-scatter æ“ä½œï¼Œå¯¹ååé‡çš„æŸå®³è¶…è¿‡äº†å…¶åœ¨å†…å­˜ä¸Šçš„å¸®åŠ©ã€‚è¿™äº›æ—©æœŸåŸºå‡†æµ‹è¯•è®©æˆ‘ä»¬å¤§å¹…ç¼©å°äº†æœç´¢ç©ºé—´ï¼Œä¸“æ³¨äºå°† Data Parallelismï¼ˆæ•°æ®å¹¶è¡Œï¼ŒDPï¼‰ ä¸é€‚åº¦çš„ Tensor Parallelismï¼ˆå¼ é‡å¹¶è¡Œï¼ŒTPï¼‰ ç›¸ç»“åˆçš„é…ç½®ã€‚

ğŸ‘‰ ä¸ºè¯„ä¼°æ¯ç§é…ç½®ï¼Œæˆ‘ä»¬è¿è¡Œ 5 æ¬¡è¿­ä»£åŸºå‡†æµ‹è¯•ï¼Œå¹¶è®°å½• tokens per second per GPU (tok/s/gpu)â€”â€”è¿™æœ€ç»ˆæ˜¯æˆ‘ä»¬å…³å¿ƒçš„æŒ‡æ ‡ã€‚æˆ‘ä»¬ä½¿ç”¨ Weights & Biases å’Œ Trackio æ¥è®°å½•ååé‡ä¸é…ç½®ï¼Œæ–¹ä¾¿æ¯”è¾ƒä¸åŒå¹¶è¡Œç­–ç•¥ã€‚

åœ¨ç³»ç»Ÿåœ°æµ‹è¯•äº† nanotron ä¸­çš„å¯ç”¨é€‰é¡¹åï¼Œæˆ‘ä»¬æœ€ç»ˆç¡®å®š DP = 192ï¼Œåˆ©ç”¨èŠ‚ç‚¹é—´ EFA å¸¦å®½è¿›è¡Œæ•°æ®å¹¶è¡Œæ¢¯åº¦åŒæ­¥ã€‚è¿™æ„å‘³ç€ 192 ä¸ªç‹¬ç«‹çš„æ¨¡å‹å‰¯æœ¬ï¼Œå„è‡ªå¤„ç†ä¸åŒçš„æ•°æ®æ‰¹æ¬¡ã€‚å¯¹äºå¼ é‡å¹¶è¡Œï¼Œæˆ‘ä»¬é€‰æ‹© TP = 2ï¼Œå°†å¼ é‡å¹¶è¡Œé€šä¿¡é™åˆ¶åœ¨å•èŠ‚ç‚¹å†…ï¼Œä»¥å……åˆ†åˆ©ç”¨ NVLink çš„é«˜å¸¦å®½ã€‚è¿™æ ·æ¯å±‚æƒé‡çŸ©é˜µè¢«æ‹†åˆ†åˆ°ä¸¤å— GPU ä¸Šï¼Œåœ¨å‰åå‘ä¼ æ’­æ—¶éœ€è¦é«˜é€Ÿé€šä¿¡ã€‚

æˆ‘ä»¬çš„ Micro Batch Size = 3ï¼ˆå¾®æ‰¹æ¬¡å¤§å° = 3ï¼‰ åœ¨å†…å­˜å ç”¨ä¸è®¡ç®—æ•ˆç‡ä¹‹é—´å–å¾—äº†å¹³è¡¡ã€‚æ›´å¤§çš„æ‰¹æ¬¡è§„æ¨¡èƒ½æ›´å¥½åœ°åˆ©ç”¨ Tensor Coresï¼ˆå¼ é‡æ ¸å¿ƒï¼‰ï¼Œä½†æˆ‘ä»¬å·²æ¥è¿‘å†…å­˜ä¸Šé™ã€‚æœ€ç»ˆï¼Œæˆ‘ä»¬é€‰æ‹©äº† ZeRO-0ï¼Œå³ä¸å¯¹ä¼˜åŒ–å™¨çŠ¶æ€åšåˆ†ç‰‡ã€‚è™½ç„¶ ZeRO-1 æˆ– ZeRO-3 å¯ä»¥è¿›ä¸€æ­¥é™ä½å†…å­˜å ç”¨ï¼Œä½†åœ¨ 384 å— GPU ä¸Šè·¨èŠ‚ç‚¹æ”¶é›†ä¸åˆ†å‘ä¼˜åŒ–å™¨çŠ¶æ€æ‰€å¸¦æ¥çš„é€šä¿¡å¼€é”€ï¼Œä¼šæ˜¾è‘—æ‹–æ…¢æ•´ä½“ååã€‚

è¯¥é…ç½®å°†å…¨å±€æ‰¹æ¬¡è§„æ¨¡æ§åˆ¶åœ¨çº¦ 200 ä¸‡ tokenï¼ˆ192 Ã— 3 Ã— 1 Ã— 4096 â‰ˆ 2.3Mï¼‰ï¼ŒåŒæ—¶åœ¨æˆ‘ä»¬ 384 å¼  H100 é›†ç¾¤ä¸Šå®ç°äº†æœ€å¤§ååã€‚å®Œæ•´è®­ç»ƒé…ç½®è§ [stage1_8T.yaml](https://github.com/huggingface/smollm/blob/main/text/pretraining/smollm3/stage1_8T.yaml)ã€‚

[Conclusion](https://huggingfacetb-smol-training-playbook.hf.space/#conclusion)
-------------------------------------------------------------------------------

æˆ‘ä»¬æœ€åˆåªé—®äº†ä¸€ä¸ªç®€å•çš„é—®é¢˜ï¼šåˆ° 2025 å¹´ï¼Œè®­ç»ƒä¸€å°é«˜æ€§èƒ½ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰åˆ°åº•éœ€è¦ä»€ä¹ˆï¼Ÿåœ¨èµ°å®Œä»é¢„è®­ç»ƒåˆ°åè®­ç»ƒçš„å®Œæ•´æµç¨‹åï¼Œæˆ‘ä»¬ä¸ä»…å±•ç¤ºäº†å…·ä½“æŠ€æœ¯ï¼Œæ›´åˆ†äº«äº†è®©è¿™äº›æŠ€æœ¯çœŸæ­£è½åœ°çš„æ•´å¥—æ–¹æ³•è®ºã€‚

Pretraining at scaleï¼ˆè§„æ¨¡åŒ–é¢„è®­ç»ƒï¼‰ã€‚ æˆ‘ä»¬ä»‹ç»äº† Training Compassï¼ˆè®­ç»ƒç½—ç›˜ï¼‰æ¡†æ¶ï¼Œç”¨æ¥åˆ¤æ–­â€œåˆ°åº•è¦ä¸è¦è®­ç»ƒâ€ï¼›éšåæ¼”ç¤ºäº†å¦‚ä½•æŠŠç›®æ ‡è½¬åŒ–ä¸ºå…·ä½“çš„æ¶æ„å†³ç­–ã€‚ä½ çœ‹åˆ°äº†å¦‚ä½•æ­å»ºå¯é çš„æ¶ˆèå®éªŒç®¡çº¿ã€å¦‚ä½•å•ç‹¬éªŒè¯æ¯é¡¹æ”¹åŠ¨ï¼Œä»¥åŠå¦‚ä½•ä»æ•°åäº¿ token çš„å°å®éªŒå¹³æ»‘æ‰©å±•åˆ°æ•°ä¸‡äº¿ token çš„å¤§è¿è¡Œã€‚æˆ‘ä»¬è®°å½•äº†è§„æ¨¡åŒ–æ—¶å¯èƒ½é‡åˆ°çš„åŸºç¡€è®¾æ–½éš¾é¢˜ï¼ˆååéª¤é™ã€æ•°æ®åŠ è½½ç“¶é¢ˆã€éšè”½ bugï¼‰ï¼Œå¹¶å±•ç¤ºäº†å¦‚ä½•é€šè¿‡ç›‘æ§ä¸ç³»ç»ŸåŒ–é™é£é™©æ‰‹æ®µå°½æ—©å‘ç°ã€å¿«é€Ÿå®šä½ã€‚

å®è·µä¸­çš„åè®­ç»ƒï¼ˆPost-trainingï¼‰ã€‚ æˆ‘ä»¬å±•ç¤ºäº†ï¼Œä»åŸºç¡€æ¨¡å‹ï¼ˆbase modelï¼‰åˆ°ç”Ÿäº§çº§åŠ©æ‰‹éœ€è¦ä¸€å¥—ç³»ç»ŸåŒ–çš„æ–¹æ³•ï¼šåœ¨è®­ç»ƒä»»ä½•å†…å®¹ä¹‹å‰å…ˆå»ºç«‹è¯„ä¼°ï¼ˆevalsï¼‰ï¼Œè¿­ä»£ç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰æ•°æ®é…æ¯”ï¼Œåº”ç”¨åå¥½ä¼˜åŒ–ï¼ˆpreference optimizationï¼‰ï¼Œå¹¶å¯é€‰æ‹©è¿›ä¸€æ­¥é€šè¿‡å¼ºåŒ–å­¦ä¹ ï¼ˆRLï¼‰æ¨è¿›ã€‚ä½ å·²ç»çœ‹åˆ°ï¼Œæ°›å›´æµ‹è¯•ï¼ˆvibe testingï¼‰å¦‚ä½•æ•æ‰åˆ°æŒ‡æ ‡é—æ¼çš„ bugï¼ŒèŠå¤©æ¨¡æ¿ï¼ˆchat templatesï¼‰å¦‚ä½•æ‚„æ— å£°æ¯åœ°ç ´åæŒ‡ä»¤éµå¾ªï¼Œä»¥åŠæ•°æ®é…æ¯”å¹³è¡¡åœ¨åè®­ç»ƒé˜¶æ®µçš„é‡è¦æ€§ä¸ºä½•ä¸é¢„è®­ç»ƒé˜¶æ®µä¸ç›¸ä¸Šä¸‹ã€‚

åœ¨æ•´ä¸ªä¸¤ä¸ªé˜¶æ®µä¸­ï¼Œæˆ‘ä»¬ä¸æ–­å›åˆ°ç›¸åŒçš„æ ¸å¿ƒæ´è§ï¼šé€šè¿‡å®éªŒéªŒè¯ä¸€åˆ‡ï¼Œä¸€æ¬¡åªæ”¹å˜ä¸€ä»¶äº‹ï¼Œé¢„æœŸè§„æ¨¡ä¼šåœ¨æ–°åœºæ™¯ä¸‹å¼•å‘é—®é¢˜ï¼Œå¹¶è®©ä½¿ç”¨åœºæ™¯é©±åŠ¨å†³ç­–ï¼Œè€Œä¸æ˜¯ç›²ç›®è¿½é€æ¯ä¸€ç¯‡æ–°è®ºæ–‡ã€‚éµå¾ªè¿™ä¸€æµç¨‹ï¼Œæˆ‘ä»¬è®­ç»ƒå‡ºäº† SmolLM3ï¼šä¸€ä¸ªå…·å¤‡ç«äº‰åŠ›çš„ 3B å¤šè¯­è¨€æ¨ç†æ¨¡å‹ï¼Œæ”¯æŒé•¿ä¸Šä¸‹æ–‡ã€‚åœ¨æ­¤è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥äº†è§£äº†å“ªäº›æ–¹æ³•æœ‰æ•ˆã€å“ªäº›ä¼šå¤±è´¥ï¼Œä»¥åŠå‡ºé”™æ—¶å¦‚ä½•è°ƒè¯•ã€‚æˆ‘ä»¬å·²å°½åŠ›è®°å½•å…¨éƒ¨ç»éªŒï¼Œæ— è®ºæˆåŠŸè¿˜æ˜¯å¤±è´¥ã€‚

ä¸‹ä¸€æ­¥ï¼Ÿ

æœ¬ç¯‡åšå®¢æ¶µç›–äº†ç°ä»£å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒçš„åŸºç¡€çŸ¥è¯†ï¼Œä½†è¯¥é¢†åŸŸå‘å±•è¿…é€Ÿã€‚ä»¥ä¸‹æ˜¯æ·±å…¥æ¢ç´¢çš„é€”å¾„ï¼š

*   äº²è‡ªè¿è¡Œå®éªŒã€‚ é˜…è¯»æ¶ˆèå®éªŒï¼ˆablationsï¼‰å›ºç„¶æœ‰ç”¨ï¼›äº²è‡ªè¿è¡Œåˆ™èƒ½è®©ä½ çœŸæ­£äº†è§£å“ªäº›å› ç´ è‡³å…³é‡è¦ã€‚é€‰ä¸€ä¸ªå°æ¨¡å‹ï¼Œå»ºç«‹è¯„ä¼°ï¼Œå¼€å§‹å®éªŒã€‚
*   é˜…è¯»æºä»£ç ã€‚ nanotronã€TRL ç­‰è®­ç»ƒæ¡†æ¶å‡ä¸ºå¼€æºã€‚æ·±å…¥å…¶å®ç°å¯æ­ç¤ºè®ºæ–‡ä¸­å¸¸è¢«å¿½ç•¥çš„ç»†èŠ‚ã€‚
*   å…³æ³¨æœ€æ–°ç ”ç©¶ã€‚ è¿‘æœŸæœ€å…ˆè¿›æ¨¡å‹çš„è®ºæ–‡å±•ç¤ºäº†é¢†åŸŸçš„å‘å±•æ–¹å‘ã€‚å‚è€ƒæ–‡çŒ®éƒ¨åˆ†æ”¶å½•äº†æˆ‘ä»¬ç²¾é€‰çš„æœ‰å½±å“åŠ›è®ºæ–‡ä¸èµ„æºæ¸…å•ã€‚

æˆ‘ä»¬å¸Œæœ›æœ¬ç¯‡åšå®¢èƒ½å¸®åŠ©ä½ åœ¨ä¸‹ä¸€æ¬¡è®­ç»ƒé¡¹ç›®ä¸­ä¿æŒæ¸…æ™°ä¸è‡ªä¿¡ï¼Œæ— è®ºä½ æ˜¯åœ¨å¤§å‹å®éªŒå®¤æ¨åŠ¨å‰æ²¿ï¼Œè¿˜æ˜¯å°å›¢é˜Ÿè§£å†³ç‰¹å®šé—®é¢˜ã€‚

ç°åœ¨å»è®­ç»ƒç‚¹ä»€ä¹ˆå§ã€‚å½“ä½ çš„æŸå¤±ï¼ˆlossï¼‰åœ¨å‡Œæ™¨ä¸¤ç‚¹ç¥ç§˜é£™å‡æ—¶ï¼Œè¯·è®°ä½ï¼šæ¯ä¸€ä¸ªä¼Ÿå¤§çš„æ¨¡å‹èƒŒåéƒ½æœ‰ä¸€å †è°ƒè¯•æ•…äº‹ã€‚æ„¿å¼€æºï¼ˆopen sourceï¼‰ä¸å¼€æ”¾ç§‘å­¦ï¼ˆopen scienceï¼‰çš„åŠ›é‡æ°¸è¿œä¸ä½ åŒåœ¨ï¼

#### [è‡´è°¢](https://huggingfacetb-smol-training-playbook.hf.space/#acknowledgments)

æˆ‘ä»¬æ„Ÿè°¢ [Guilherme](https://huggingface.co/guipenedo)ã€[Hugo](https://huggingface.co/hlarcher) å’Œ [Mario](https://huggingface.co/mariolr) æä¾›çš„å®è´µåé¦ˆï¼Œä»¥åŠ [Abubakar](https://huggingface.co/abidlabs) åœ¨ Trackio åŠŸèƒ½æ–¹é¢ç»™äºˆçš„å¸®åŠ©ã€‚

[å‚è€ƒæ–‡çŒ®](https://huggingfacetb-smol-training-playbook.hf.space/#references)
-------------------------------------------------------------------------------

ä»¥ä¸‹æ˜¯æˆ‘ä»¬ç²¾å¿ƒæ•´ç†çš„è®ºæ–‡ã€ä¹¦ç±å’Œåšå®¢æ–‡ç« åˆ—è¡¨ï¼Œå®ƒä»¬åœ¨æˆ‘ä»¬çš„ LLMï¼ˆå¤§è¯­è¨€æ¨¡å‹ï¼‰è®­ç»ƒä¹‹æ—…ä¸­ç»™äºˆäº†æˆ‘ä»¬æœ€å¤§çš„å¯å‘ã€‚

#### [LLM æ¶æ„](https://huggingfacetb-smol-training-playbook.hf.space/#llm-architecture)

*   ç¨ å¯†æ¨¡å‹ï¼ˆDense modelsï¼‰ï¼š[Llama3](https://huggingface.co/papers/2407.21783)ã€[Olmo2](https://huggingface.co/papers/2501.00656)ã€[MobileLLM](https://huggingface.co/papers/2402.14905)
*   MoEï¼ˆæ··åˆä¸“å®¶æ¨¡å‹ï¼ŒMixture of Expertsï¼‰ï¼š[DeepSeek V2](https://huggingface.co/papers/2405.04434)ã€[DeepSeek V3](https://huggingface.co/papers/2412.19437)ã€[Scaling Laws of Efficient MoEs](https://huggingface.co/papers/2507.17702)
*   æ··åˆæ¶æ„ï¼ˆHybridï¼‰ï¼š[MiniMax-01](https://huggingface.co/papers/2501.08313)ã€[Mamba2](https://huggingface.co/papers/2405.21060)

#### [ä¼˜åŒ–å™¨ä¸è®­ç»ƒå‚æ•°](https://huggingfacetb-smol-training-playbook.hf.space/#optimisers--training-parameters)

*   [Muon is Scalable for LLM Training](https://huggingface.co/papers/2502.16982)ã€[Fantastic pretraining optimisers](https://huggingface.co/papers/2509.02046)
*   [Large Batch Training](https://arxiv.org/abs/1812.06162)ã€[DeepSeekLLM](https://arxiv.org/abs/2401.02954)

#### [æ•°æ®æ•´ç†ï¼ˆData curationï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#data-curation)

*   ç½‘é¡µï¼š [FineWeb & FineWeb-Edu](https://huggingface.co/papers/2406.17557)ã€[FineWeb2](https://huggingface.co/papers/2506.20920)ã€[DCLM](https://huggingface.co/papers/2406.11794)
*   ä»£ç ï¼š [The Stack v2](https://huggingface.co/papers/2402.19173)ã€[To Code or Not to Code](https://huggingface.co/papers/2408.10914)
*   æ•°å­¦ï¼š [DeepSeekMath](https://huggingface.co/papers/2402.03300)ã€[FineMath](https://huggingface.co/papers/2502.02737)ã€[MegaMath](https://huggingface.co/papers/2504.02807)
*   æ•°æ®æ··åˆï¼š [SmolLM2](https://huggingface.co/papers/2502.02737)ã€[Does your data spark joy](https://huggingface.co/papers/2406.03476)

#### [æ‰©å±•å®šå¾‹ï¼ˆScaling lawsï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#scaling-laws)

*   [Kaplan](https://huggingface.co/papers/2001.08361)ã€[Chinchilla](https://huggingface.co/papers/2203.15556)ã€[Scaling Data-Constrained Language Models](https://huggingface.co/papers/2305.16264)

#### [åè®­ç»ƒï¼ˆPost-trainingï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#post-training)

*   [InstructGPT:](https://huggingface.co/papers/2203.02155) OpenAI çš„å¼€å±±ä¹‹ä½œï¼Œå°†åŸºç¡€æ¨¡å‹è½¬åŒ–ä¸ºæœ‰ç”¨åŠ©æ‰‹ã€‚ChatGPT çš„å‰èº«ï¼Œä¹Ÿæ˜¯äººç±»æ”€ç™»å¡å°”è¾¾èˆå¤«ï¼ˆKardashevï¼‰ç­‰çº§ä¹‹è·¯ä¸Šçš„å…³é”®ä¸€æ­¥ã€‚
*   [Llama 2](https://huggingface.co/papers/2307.09288) ä¸ [3](https://huggingface.co/papers/2407.21783)ï¼šMeta å‘å¸ƒçš„æå…¶è¯¦å°½çš„æŠ€æœ¯æŠ¥å‘Šï¼Œæ­ç§˜ Llama æ¨¡å‹èƒŒåçš„è®­ç»ƒç»†èŠ‚ï¼ˆæ„¿å®ƒä»¬å®‰æ¯ï¼‰ã€‚ä¸¤ç¯‡æŠ¥å‘Šéƒ½åŒ…å«å¤§é‡å…³äºäººç±»æ•°æ®æ”¶é›†çš„æ´è§ï¼Œæ¶µç›–äººç±»åå¥½ä¸æ¨¡å‹è¯„ä¼°ã€‚
*   Secrets of RLHF in LLMsï¼Œ[ç¬¬ä¸€éƒ¨åˆ†](https://huggingface.co/papers/2307.04964) ä¸ [ç¬¬äºŒéƒ¨åˆ†](https://huggingface.co/papers/2401.06080)ï¼šè¿™ä¸¤ç¯‡è®ºæ–‡æ»¡æ»¡éƒ½æ˜¯ RLHFï¼ˆReinforcement Learning from Human Feedbackï¼Œäººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ ï¼‰çš„å®æ“ç»†èŠ‚ï¼Œå°¤å…¶æ˜¯å¦‚ä½•è®­ç»ƒå¼ºå¤§çš„å¥–åŠ±æ¨¡å‹ã€‚
*   [Direct Preference Optimisation:](https://huggingface.co/papers/2305.18290) 2023 å¹´çš„çªç ´æ€§è®ºæ–‡ï¼Œè®©æ‰€æœ‰äººä¸å†å¯¹ LLMï¼ˆLarge Language Modelï¼Œå¤§è¯­è¨€æ¨¡å‹ï¼‰åš RLï¼ˆReinforcement Learningï¼Œå¼ºåŒ–å­¦ä¹ ï¼‰ã€‚
*   [DeepSeek-R1:](https://huggingface.co/papers/2501.12948) 2025 å¹´çš„çªç ´æ€§è®ºæ–‡ï¼Œåˆè®©æ‰€æœ‰äººé‡æ–°å¼€å§‹å¯¹ LLM åš RLã€‚
*   [Dr. GRPO:](https://huggingface.co/papers/2503.20783) ç†è§£ GRPOï¼ˆGroup Relative Policy Optimizationï¼Œç¾¤ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼‰å†…åœ¨åå·®åŠå…¶ä¿®æ­£æ–¹æ³•çš„æœ€é‡è¦è®ºæ–‡ä¹‹ä¸€ã€‚
*   [DAPO:](https://huggingface.co/papers/2503.14476) å­—èŠ‚è·³åŠ¨åˆ†äº«å¤§é‡å®ç°ç»†èŠ‚ï¼Œä¸ºç¤¾åŒºè§£é”ç¨³å®šçš„ R1-Zero å¼è®­ç»ƒã€‚
*   [ScaleRL:](https://huggingface.co/papers/2510.13786) Meta çš„â€œè‚Œè‚‰ç§€â€ï¼Œæ¨å¯¼å‡º RL çš„æ‰©å±•å®šå¾‹ï¼ˆscaling lawsï¼‰ã€‚çƒ§æ‰ 40 ä¸‡ GPU å°æ—¶ï¼Œç¡®ç«‹ä¸€å¥—åœ¨å¤šä¸ªæ•°é‡çº§ç®—åŠ›ä¸Šéƒ½å¯å¯é æ‰©å±•çš„è®­ç»ƒé…æ–¹ã€‚
*   [LoRA without Regret:](https://thinkingmachines.ai/blog/lora/) ä¸€ç¯‡æ–‡ç¬”ä¼˜ç¾çš„åšå®¢ï¼Œå‘ç°ä½ç§© LoRAï¼ˆLow-Rank Adaptationï¼Œä½ç§©é€‚é…ï¼‰ä¹Ÿèƒ½åœ¨ RL ä¸­åª²ç¾å…¨å‚æ•°å¾®è°ƒï¼ˆæœ€ä»¤äººæƒŠå–œçš„ç»“æœï¼‰ã€‚
*   [Command A:](https://huggingface.co/papers/2504.00698) Cohere å‘å¸ƒçš„ä¸€ä»½å¼‚å¸¸è¯¦å°½çš„æŠ€æœ¯æŠ¥å‘Šï¼Œä»‹ç»å¤šç§é«˜æ•ˆåè®­ç»ƒ LLM çš„ç­–ç•¥ã€‚

#### [åŸºç¡€è®¾æ–½](https://huggingfacetb-smol-training-playbook.hf.space/#infrastructure)

*   [UltraScale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook)
*   [Jax scaling book](https://jax-ml.github.io/scaling-book/)
*   [Modal GPU Glossary](https://modal.com/gpu-glossary/readme)

#### [è®­ç»ƒæ¡†æ¶](https://huggingfacetb-smol-training-playbook.hf.space/#training-frameworks)

*   [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
*   [DeepSpeed](https://github.com/deepspeedai/DeepSpeed)
*   [Torchtitan](https://github.com/pytorch/torchtitan)
*   [Nanotron](https://github.com/huggingface/nanotron/)
*   [NanoChat](https://github.com/karpathy/nanochat)
*   [TRL](https://github.com/huggingface/trl)

#### [è¯„ä¼°ï¼ˆEvaluationï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#evaluation)

*   [LLM è¯„ä¼°æŒ‡å—ï¼ˆThe LLM Evaluation Guidebookï¼‰](https://github.com/huggingface/evaluation-guidebook)
*   [OLMES](https://huggingface.co/papers/2406.08446)
*   [FineTasks](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fine-tasks)
*   [æ¥è‡ªä¸€çº¿çš„ç»éªŒæ•™è®­ï¼ˆLessons from the trenchesï¼‰](https://huggingface.co/papers/2405.14782)

[è„šæ³¨ï¼ˆFootnotesï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/#footnote-label)
----------------------------------------------------------------------------------

1.   è®¡ç®—è¿™äº›ç»Ÿè®¡é‡çš„æƒ³æ³•æ¥è‡ª Llama 3 æŠ€æœ¯æŠ¥å‘Šï¼ˆ[Grattafiori et al., 2024](https://arxiv.org/abs/2407.21783)ï¼‰ã€‚

[](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f1)
2.   å…³äº vLLMï¼Œå‚è§ï¼š[æ¨ç†è§£æå™¨ï¼ˆReasoning parsersï¼‰](https://docs.vllm.ai/en/v0.10.1.1/features/reasoning_outputs.html)ã€[å·¥å…·è§£æå™¨ï¼ˆTool parsersï¼‰](https://huggingfacetb-smol-training-playbook.hf.space/2421384ebcac80fbaa7cf939fc39269d)ã€‚å…³äº SGLangï¼Œå‚è§ï¼š[æ¨ç†è§£æå™¨ï¼ˆReasoning parsersï¼‰](https://docs.sglang.ai/advanced_features/separate_reasoning.html)ã€[å·¥å…·è§£æå™¨ï¼ˆTool parsersï¼‰](https://docs.sglang.ai/advanced_features/tool_parser.html)

[](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f2)
3.   Transformers å›¢é˜Ÿæœ€è¿‘æ–°å¢äº†è§£æå™¨ï¼ˆ[parsers](https://huggingface.co/docs/transformers/main/en/chat_response_parsing)ï¼‰ï¼Œç”¨äºæå–å·¥å…·è°ƒç”¨ï¼ˆtool callingï¼‰å’Œæ¨ç†è¾“å‡ºï¼ˆreasoning outputsï¼‰ã€‚å¦‚æœåƒ vLLM è¿™æ ·çš„å¼•æ“é‡‡ç”¨è¿™äº›è§£æå™¨ï¼Œå…¼å®¹æ€§æ ‡å‡†åœ¨æœªæ¥å¯èƒ½å˜å¾—ä¸é‚£ä¹ˆé‡è¦ã€‚

[](https://huggingfacetb-smol-training-playbook.hf.space/#user-content-fnref-f3)


1.   Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Ramos, S., Geist, M., & Bachem, O. (2024). _On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes_. [https://arxiv.org/abs/2306.13649](https://arxiv.org/abs/2306.13649)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gkd-1)
2.   Ainslie, J., Lee-Thorp, J., de Jong, M., Zemlyanskiy, Y., LebrÃ³n, F., & Sanghai, S. (2023). _GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints_. [https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gqa-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gqa-2)
3.   Allal, L. B., Lozhkov, A., Bakouch, E., BlÃ¡zquez, G. M., Penedo, G., Tunstall, L., Marafioti, A., KydlÃ­Äek, H., LajarÃ­n, A. P., Srivastav, V., Lochner, J., Fahlgren, C., Nguyen, X.-S., Fourrier, C., Burtenshaw, B., Larcher, H., Zhao, H., Zakka, C., Morlon, M., â€¦ Wolf, T. (2025). _SmolLM2: When Smol Goes Big â€“ Data-Centric Training of a Small Language Model_. [https://arxiv.org/abs/2502.02737](https://arxiv.org/abs/2502.02737) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smollm2-3)
4.   Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, Ã‰., Hesslow, D., Launay, J., Malartic, Q., Mazzotta, D., Noune, B., Pannier, B., & Penedo, G. (2023). _The Falcon Series of Open Language Models_. [https://arxiv.org/abs/2311.16867](https://arxiv.org/abs/2311.16867)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-almazrouei2023falconseriesopenlanguage-1)
5.   An, C., Huang, F., Zhang, J., Gong, S., Qiu, X., Zhou, C., & Kong, L. (2024). _Training-Free Long-Context Scaling of Large Language Models_. [https://arxiv.org/abs/2402.17463](https://arxiv.org/abs/2402.17463)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dca-1)
6.   Aryabumi, V., Su, Y., Ma, R., Morisot, A., Zhang, I., Locatelli, A., Fadaee, M., ÃœstÃ¼n, A., & Hooker, S. (2024). _To Code, or Not To Code? Exploring Impact of Code in Pre-training_. [https://arxiv.org/abs/2408.10914](https://arxiv.org/abs/2408.10914)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-aryabumi2024codecodeexploringimpact-1)
7.   Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., Hui, B., Ji, L., Li, M., Lin, J., Lin, R., Liu, D., Liu, G., Lu, C., Lu, K., â€¦ Zhu, T. (2023). _Qwen Technical Report_. [https://arxiv.org/abs/2309.16609](https://arxiv.org/abs/2309.16609)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1-1)
8.   Barres, V., Dong, H., Ray, S., Si, X., & Narasimhan, K. (2025). _Ï„ 2-Bench: Evaluating Conversational Agents in a Dual-Control Environment_. [https://arxiv.org/abs/2506.07982](https://arxiv.org/abs/2506.07982)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-taubench-1)
9.   Beck, M., PÃ¶ppel, K., Lippe, P., & Hochreiter, S. (2025). _Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels_. [https://arxiv.org/abs/2503.14376](https://arxiv.org/abs/2503.14376)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-beck2025tiledflashlinearattention-1)
10.   Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., â€¦ Amodei, D. (2020). _Language Models are Few-Shot Learners_. [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt3-1)
11.   Chen, M., Tworek, J., Jun, H., Yuan, Q., de Oliveira Pinto, H. P., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf, H., Sastry, G., Mishkin, P., Chan, B., Gray, S., â€¦ Zaremba, W. (2021). _Evaluating Large Language Models Trained on Code_. [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-codex-1)
12.   Chen, Y., Huang, B., Gao, Y., Wang, Z., Yang, J., & Ji, H. (2025a). _Scaling Laws for Predicting Downstream Performance in LLMs_. [https://arxiv.org/abs/2410.08527](https://arxiv.org/abs/2410.08527)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chen2025-1)
13.   Chen, Y., Huang, B., Gao, Y., Wang, Z., Yang, J., & Ji, H. (2025b). _Scaling Laws for Predicting Downstream Performance in LLMs_. [https://arxiv.org/abs/2410.08527](https://arxiv.org/abs/2410.08527)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chen2025scalinglawspredictingdownstream-1)
14.   Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating long sequences with sparse transformers. _arXiv Preprint arXiv:1904.10509_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-child2019generating-1)
15.   Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., Schuh, P., Shi, K., Tsvyashchenko, S., Maynez, J., Rao, A., Barnes, P., Tay, Y., Shazeer, N., Prabhakaran, V., â€¦ Fiedel, N. (2022). _PaLM: Scaling Language Modeling with Pathways_. [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-3), [4](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-palm-4)
16.   Chu, T., Zhai, Y., Yang, J., Tong, S., Xie, S., Schuurmans, D., Le, Q. V., Levine, S., & Ma, Y. (2025). _SFT Memorizes, RL Generalizes: A Comparative Study of Foundation Model Post-training_. [https://arxiv.org/abs/2501.17161](https://arxiv.org/abs/2501.17161)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-chu2025-1)
17.   Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., & Schulman, J. (2021). _Training Verifiers to Solve Math Word Problems_. [https://arxiv.org/abs/2110.14168](https://arxiv.org/abs/2110.14168)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gsm8k-1)
18.   Cohere, T., :, Aakanksha, Ahmadian, A., Ahmed, M., Alammar, J., Alizadeh, M., Alnumay, Y., Althammer, S., Arkhangorodsky, A., Aryabumi, V., Aumiller, D., Avalos, R., Aviv, Z., Bae, S., Baji, S., Barbet, A., Bartolo, M., Bebensee, B., â€¦ Zhao, Z. (2025). _Command A: An Enterprise-Ready Large Language Model_. [https://arxiv.org/abs/2504.00698](https://arxiv.org/abs/2504.00698)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-commandacohere-1)
19.   Dagan, G., Synnaeve, G., & RoziÃ¨re, B. (2024). _Getting the most out of your tokenizer for pre-training and domain adaptation_. [https://arxiv.org/abs/2402.01035](https://arxiv.org/abs/2402.01035) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dagan2024gettingtokenizerpretrainingdomain-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dagan2024gettingtokenizerpretrainingdomain-2)
20.   Dao, T., & Gu, A. (2024). _Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality_. [https://arxiv.org/abs/2405.21060](https://arxiv.org/abs/2405.21060) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba2-2)
21.   DeepSeek-AI. (2025). _DeepSeek-V3.2-Exp: Boosting Long-Context Efficiency with DeepSeek Sparse Attention_. DeepSeek. [https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf](https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dsa-1)
22.   DeepSeek-AI, :, Bi, X., Chen, D., Chen, G., Chen, S., Dai, D., Deng, C., Ding, H., Dong, K., Du, Q., Fu, Z., Gao, H., Gao, K., Gao, W., Ge, R., Guan, K., Guo, D., Guo, J., â€¦ Zou, Y. (2024). _DeepSeek LLM: Scaling Open-Source Language Models with Longtermism_. [https://arxiv.org/abs/2401.02954](https://arxiv.org/abs/2401.02954) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekai2024deepseekllmscalingopensource-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekai2024deepseekllmscalingopensource-2)
23.   DeepSeek-AI, Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., Zhang, X., Yu, X., Wu, Y., Wu, Z. F., Gou, Z., Shao, Z., Li, Z., Gao, Z., â€¦ Zhang, Z. (2025). _DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning_. [https://arxiv.org/abs/2501.12948](https://arxiv.org/abs/2501.12948)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekr1-1)
24.   DeepSeek-AI, Liu, A., Feng, B., Wang, B., Wang, B., Liu, B., Zhao, C., Dengr, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Luo, F., Hao, G., Chen, G., â€¦ Xie, Z. (2024). _DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model_. [https://arxiv.org/abs/2405.04434](https://arxiv.org/abs/2405.04434) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv2-2)
25.   DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B., Lu, C., Zhao, C., Deng, C., Zhang, C., Ruan, C., Dai, D., Guo, D., Yang, D., Chen, D., Ji, D., Li, E., Lin, F., Dai, F., â€¦ Pan, Z. (2025). _DeepSeek-V3 Technical Report_. [https://arxiv.org/abs/2412.19437](https://arxiv.org/abs/2412.19437)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-deepseekv3-1)
26.   Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., Steiner, A., Caron, M., Geirhos, R., Alabdulmohsin, I., Jenatton, R., Beyer, L., Tschannen, M., Arnab, A., Wang, X., Riquelme, C., Minderer, M., Puigcerver, J., Evci, U., â€¦ Houlsby, N. (2023). _Scaling Vision Transformers to 22 Billion Parameters_. [https://arxiv.org/abs/2302.05442](https://arxiv.org/abs/2302.05442)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dehghani2023scalingvisiontransformers22-1)
27.   Ding, H., Wang, Z., Paolini, G., Kumar, V., Deoras, A., Roth, D., & Soatto, S. (2024). _Fewer Truncations Improve Language Modeling_. [https://arxiv.org/abs/2404.10830](https://arxiv.org/abs/2404.10830)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-bfd-1)
28.   Dâ€™Oosterlinck, K., Xu, W., Develder, C., Demeester, T., Singh, A., Potts, C., Kiela, D., & Mehri, S. (2024). _Anchored Preference Optimization and Contrastive Revisions: Addressing Underspecification in Alignment_. [https://arxiv.org/abs/2408.06266](https://arxiv.org/abs/2408.06266)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-apo-1)
29.   Du, Z., Zeng, A., Dong, Y., & Tang, J. (2025). _Understanding Emergent Abilities of Language Models from the Loss Perspective_. [https://arxiv.org/abs/2403.15796](https://arxiv.org/abs/2403.15796) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-du2025-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-du2025-2)
30.   Dubois, Y., Galambosi, B., Liang, P., & Hashimoto, T. B. (2025). _Length-Controlled AlpacaEval: A Simple Way to Debias Automatic Evaluators_. [https://arxiv.org/abs/2404.04475](https://arxiv.org/abs/2404.04475)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-alpacaeval-1)
31.   Ethayarajh, K., Xu, W., Muennighoff, N., Jurafsky, D., & Kiela, D. (2024). _KTO: Model Alignment as Prospect Theoretic Optimization_. [https://arxiv.org/abs/2402.01306](https://arxiv.org/abs/2402.01306)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kto-1)
32.   Gandhi, K., Chakravarthy, A., Singh, A., Lile, N., & Goodman, N. D. (2025). _Cognitive Behaviors that Enable Self-Improving Reasoners, or, Four Habits of Highly Effective STaRs_. [https://arxiv.org/abs/2503.01307](https://arxiv.org/abs/2503.01307)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-cognitivebehaviours-1)
33.   Gao, T., Wettig, A., Yen, H., & Chen, D. (2025). _How to Train Long-Context Language Models (Effectively)_. [https://arxiv.org/abs/2410.02660](https://arxiv.org/abs/2410.02660) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-prolong-3)
34.   Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., â€¦ Ma, Z. (2024). _The Llama 3 Herd of Models_. [https://arxiv.org/abs/2407.21783](https://arxiv.org/abs/2407.21783) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama3-3)
35.   Gu, A., & Dao, T. (2024). _Mamba: Linear-Time Sequence Modeling with Selective State Spaces_. [https://arxiv.org/abs/2312.00752](https://arxiv.org/abs/2312.00752)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mamba-1)
36.   Gu, Y., Tafjord, O., Kuehl, B., Haddad, D., Dodge, J., & Hajishirzi, H. (2025). _OLMES: A Standard for Language Model Evaluations_. [https://arxiv.org/abs/2406.08446](https://arxiv.org/abs/2406.08446) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmes-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmes-2)
37.   Guo, S., Zhang, B., Liu, T., Liu, T., Khalman, M., Llinares, F., Rame, A., Mesnard, T., Zhao, Y., Piot, B., Ferret, J., & Blondel, M. (2024). _Direct Language Model Alignment from Online AI Feedback_. [https://arxiv.org/abs/2402.04792](https://arxiv.org/abs/2402.04792)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-onlinedpo-1)
38.   HÃ¤gele, A., Bakouch, E., Kosson, A., Allal, L. B., Werra, L. V., & Jaggi, M. (2024). _Scaling Laws and Compute-Optimal Training Beyond Fixed Training Durations_. [https://arxiv.org/abs/2405.18392](https://arxiv.org/abs/2405.18392) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wsdhagele-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wsdhagele-2)
39.   He, Y., Jin, D., Wang, C., Bi, C., Mandyam, K., Zhang, H., Zhu, C., Li, N., Xu, T., Lv, H., Bhosale, S., Zhu, C., Sankararaman, K. A., Helenowski, E., Kambadur, M., Tayade, A., Ma, H., Fang, H., & Wang, S. (2024). _Multi-IF: Benchmarking LLMs on Multi-Turn and Multilingual Instructions Following_. [https://arxiv.org/abs/2410.15553](https://arxiv.org/abs/2410.15553)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-multiif-1)
40.   Hoffmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Rutherford, E., de Las Casas, D., Hendricks, L. A., Welbl, J., Clark, A., Hennigan, T., Noland, E., Millican, K., van den Driessche, G., Damoc, B., Guy, A., Osindero, S., Simonyan, K., Elsen, E., â€¦ Sifre, L. (2022). _Training Compute-Optimal Large Language Models_. [https://arxiv.org/abs/2203.15556](https://arxiv.org/abs/2203.15556)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-hoffmann2022trainingcomputeoptimallargelanguage-1)
41.   Hong, J., Lee, N., & Thorne, J. (2024). _ORPO: Monolithic Preference Optimization without Reference Model_. [https://arxiv.org/abs/2403.07691](https://arxiv.org/abs/2403.07691)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-orpo-1)
42.   Howard, J., & Ruder, S. (2018). _Universal Language Model Fine-tuning for Text Classification_. [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ulmfit-1)
43.   Hsieh, C.-P., Sun, S., Kriman, S., Acharya, S., Rekesh, D., Jia, F., Zhang, Y., & Ginsburg, B. (2024). _RULER: Whatâ€™s the Real Context Size of Your Long-Context Language Models?_[https://arxiv.org/abs/2404.06654](https://arxiv.org/abs/2404.06654) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ruler-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ruler-2)
44.   Hu, S., Tu, Y., Han, X., He, C., Cui, G., Long, X., Zheng, Z., Fang, Y., Huang, Y., Zhao, W., Zhang, X., Thai, Z. L., Zhang, K., Wang, C., Yao, Y., Zhao, C., Zhou, J., Cai, J., Zhai, Z., â€¦ Sun, M. (2024). _MiniCPM: Unveiling the Potential of Small Language Models with Scalable Training Strategies_. [https://arxiv.org/abs/2404.06395](https://arxiv.org/abs/2404.06395)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-hu2024minicpmunveilingpotentialsmall-1)
45.   Huang, S., Noukhovitch, M., Hosseini, A., Rasul, K., Wang, W., & Tunstall, L. (2024). _The N+ Implementation Details of RLHF with PPO: A Case Study on TL;DR Summarization_. [https://arxiv.org/abs/2403.17031](https://arxiv.org/abs/2403.17031)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ndetailsrlhf-1)
46.   IBM Research. (2025). _IBM Granite 4.0: Hyper-efficient, High Performance Hybrid Models for Enterprise_. [https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models](https://www.ibm.com/new/announcements/ibm-granite-4-0-hyper-efficient-high-performance-hybrid-models)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-granite4-1)
47.   Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., de las Casas, D., Bressand, F., Lengyel, G., Lample, G., Saulnier, L., Lavaud, L. R., Lachaux, M.-A., Stock, P., Scao, T. L., Lavril, T., Wang, T., Lacroix, T., & Sayed, W. E. (2023). _Mistral 7B_. [https://arxiv.org/abs/2310.06825](https://arxiv.org/abs/2310.06825)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-jiang2023mistral7b-1)
48.   Kamradt, G. (2023). Needle In A Haystack - pressure testing LLMs. In _GitHub repository_. GitHub. [https://github.com/gkamradt/LLMTest_NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-niah-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-niah-2)
49.   Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., & Amodei, D. (2020). _Scaling Laws for Neural Language Models_. [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kaplan2020scalinglawsneurallanguage-1)
50.   Katsch, T. (2024). _GateLoop: Fully Data-Controlled Linear Recurrence for Sequence Modeling_. [https://arxiv.org/abs/2311.01927](https://arxiv.org/abs/2311.01927)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-katsch2024gateloopfullydatacontrolledlinear-1)
51.   Kazemnejad, A., Padhi, I., Ramamurthy, K. N., Das, P., & Reddy, S. (2023). _The Impact of Positional Encoding on Length Generalization in Transformers_. [https://arxiv.org/abs/2305.19466](https://arxiv.org/abs/2305.19466)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nope-1)
52.   Khatri, D., Madaan, L., Tiwari, R., Bansal, R., Duvvuri, S. S., Zaheer, M., Dhillon, I. S., Brandfonbrener, D., & Agarwal, R. (2025). _The Art of Scaling Reinforcement Learning Compute for LLMs_. [https://arxiv.org/abs/2510.13786](https://arxiv.org/abs/2510.13786) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-scalerl-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-scalerl-2)
53.   Kingma, D. P. (2014). Adam: A method for stochastic optimization. _arXiv Preprint arXiv:1412.6980_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kingma2014adam-1)
54.   Krajewski, J., Ludziejewski, J., Adamczewski, K., PiÃ³ro, M., Krutul, M., Antoniak, S., Ciebiera, K., KrÃ³l, K., OdrzygÃ³ÅºdÅº, T., Sankowski, P., Cygan, M., & Jaszczur, S. (2024). _Scaling Laws for Fine-Grained Mixture of Experts_. [https://arxiv.org/abs/2402.07871](https://arxiv.org/abs/2402.07871)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-krajewski2024scalinglawsfinegrainedmixture-1)
55.   Lambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022). Illustrating Reinforcement Learning from Human Feedback (RLHF). _Hugging Face Blog_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rlhf-1)
56.   Lambert, N., Morrison, J., Pyatkin, V., Huang, S., Ivison, H., Brahman, F., Miranda, L. J. V., Liu, A., Dziri, N., Lyu, S., Gu, Y., Malik, S., Graf, V., Hwang, J. D., Yang, J., Bras, R. L., Tafjord, O., Wilhelm, C., Soldaini, L., â€¦ Hajishirzi, H. (2025). _Tulu 3: Pushing Frontiers in Open Language Model Post-Training_. [https://arxiv.org/abs/2411.15124](https://arxiv.org/abs/2411.15124)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-tulu3-1)
57.   Lanchantin, J., Chen, A., Lan, J., Li, X., Saha, S., Wang, T., Xu, J., Yu, P., Yuan, W., Weston, J. E., Sukhbaatar, S., & Kulikov, I. (2025). _Bridging Offline and Online Reinforcement Learning for LLMs_. [https://arxiv.org/abs/2506.21495](https://arxiv.org/abs/2506.21495)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-online-offline-1)
58.   Li, J., Fang, A., Smyrnis, G., Ivgi, M., Jordan, M., Gadre, S., Bansal, H., Guha, E., Keh, S., Arora, K., Garg, S., Xin, R., Muennighoff, N., Heckel, R., Mercat, J., Chen, M., Gururangan, S., Wortsman, M., Albalak, A., â€¦ Shankar, V. (2025). _DataComp-LM: In search of the next generation of training sets for language models_. [https://arxiv.org/abs/2406.11794](https://arxiv.org/abs/2406.11794)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-datacomp-1)
59.   Li, Q., Cui, L., Zhao, X., Kong, L., & Bi, W. (2024). _GSM-Plus: A Comprehensive Benchmark for Evaluating the Robustness of LLMs as Mathematical Problem Solvers_. [https://arxiv.org/abs/2402.19255](https://arxiv.org/abs/2402.19255)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gsmplus-1)
60.   Li, R., Allal, L. B., Zi, Y., Muennighoff, N., Kocetkov, D., Mou, C., Marone, M., Akiki, C., Li, J., Chim, J., Liu, Q., Zheltonozhskii, E., Zhuo, T. Y., Wang, T., Dehaene, O., Davaadorj, M., Lamy-Poirier, J., Monteiro, J., Shliazhko, O., â€¦ de Vries, H. (2023). _StarCoder: may the source be with you!_[https://arxiv.org/abs/2305.06161](https://arxiv.org/abs/2305.06161)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-starcoder-1)
61.   Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., & Stoica, I. (2024). _From Crowdsourced Data to High-Quality Benchmarks: Arena-Hard and BenchBuilder Pipeline_. [https://arxiv.org/abs/2406.11939](https://arxiv.org/abs/2406.11939)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-arenahard-1)
62.   Liang, W., Liu, T., Wright, L., Constable, W., Gu, A., Huang, C.-C., Zhang, I., Feng, W., Huang, H., Wang, J., Purandare, S., Nadathur, G., & Idreos, S. (2025). _TorchTitan: One-stop PyTorch native solution for production ready LLM pre-training_. [https://arxiv.org/abs/2410.06511](https://arxiv.org/abs/2410.06511)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-torchtitan-1)
63.   Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., & Cobbe, K. (2023). _Letâ€™s Verify Step by Step_. [https://arxiv.org/abs/2305.20050](https://arxiv.org/abs/2305.20050)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-openaiprm-1)
64.   Liu, H., Xie, S. M., Li, Z., & Ma, T. (2022). _Same Pre-training Loss, Better Downstream: Implicit Bias Matters for Language Models_. [https://arxiv.org/abs/2210.14199](https://arxiv.org/abs/2210.14199)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-liu2022-1)
65.   Liu, Q., Zheng, X., Muennighoff, N., Zeng, G., Dou, L., Pang, T., Jiang, J., & Lin, M. (2025). _RegMix: Data Mixture as Regression for Language Model Pre-training_. [https://arxiv.org/abs/2407.01492](https://arxiv.org/abs/2407.01492)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-liu2025regmixdatamixtureregression-1)
66.   Liu, Z., Zhao, C., Iandola, F., Lai, C., Tian, Y., Fedorov, I., Xiong, Y., Chang, E., Shi, Y., Krishnamoorthi, R., Lai, L., & Chandra, V. (2024). _MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases_. [https://arxiv.org/abs/2402.14905](https://arxiv.org/abs/2402.14905)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mobilellm-1)
67.   Loshchilov, I., & Hutter, F. (2017). _SGDR: Stochastic Gradient Descent with Warm Restarts_. [https://arxiv.org/abs/1608.03983](https://arxiv.org/abs/1608.03983)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-loshchilov2017sgdrstochasticgradientdescent-1)
68.   Lozhkov, A., Li, R., Allal, L. B., Cassano, F., Lamy-Poirier, J., Tazi, N., Tang, A., Pykhtar, D., Liu, J., Wei, Y., Liu, T., Tian, M., Kocetkov, D., Zucker, A., Belkada, Y., Wang, Z., Liu, Q., Abulkhanov, D., Paul, I., â€¦ de Vries, H. (2024). _StarCoder 2 and The Stack v2: The Next Generation_. [https://arxiv.org/abs/2402.19173](https://arxiv.org/abs/2402.19173)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-starcoder2-1)
69.   Mao, H. H. (2022). _Fine-Tuning Pre-trained Transformers into Decaying Fast Weights_. [https://arxiv.org/abs/2210.04243](https://arxiv.org/abs/2210.04243)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mao2022finetuningpretrainedtransformersdecaying-1)
70.   Marafioti, A., Zohar, O., FarrÃ©, M., Noyan, M., Bakouch, E., Cuenca, P., Zakka, C., Allal, L. B., Lozhkov, A., Tazi, N., Srivastav, V., Lochner, J., Larcher, H., Morlon, M., Tunstall, L., von Werra, L., & Wolf, T. (2025). _SmolVLM: Redefining small and efficient multimodal models_. [https://arxiv.org/abs/2504.05299](https://arxiv.org/abs/2504.05299)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smolvlm-1)
71.   McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). _An Empirical Model of Large-Batch Training_. [https://arxiv.org/abs/1812.06162](https://arxiv.org/abs/1812.06162)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mccandlish2018empiricalmodellargebatchtraining-1)
72.   Merrill, W., Arora, S., Groeneveld, D., & Hajishirzi, H. (2025). _Critical Batch Size Revisited: A Simple Empirical Approach to Large-Batch Language Model Training_. [https://arxiv.org/abs/2505.23971](https://arxiv.org/abs/2505.23971)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-merrill2025criticalbatchsizerevisited-1)
73.   Meta AI. (2025). _The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation_. [https://ai.meta.com/blog/llama-4-multimodal-intelligence/](https://ai.meta.com/blog/llama-4-multimodal-intelligence/) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama4-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-llama4-2)
74.   Mindermann, S., Brauner, J., Razzak, M., Sharma, M., Kirsch, A., Xu, W., HÃ¶ltgen, B., Gomez, A. N., Morisot, A., Farquhar, S., & Gal, Y. (2022). _Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt_. [https://arxiv.org/abs/2206.07137](https://arxiv.org/abs/2206.07137)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mindermann2022prioritizedtrainingpointslearnable-1)
75.   MiniMax, Li, A., Gong, B., Yang, B., Shan, B., Liu, C., Zhu, C., Zhang, C., Guo, C., Chen, D., Li, D., Jiao, E., Li, G., Zhang, G., Sun, H., Dong, H., Zhu, J., Zhuang, J., Song, J., â€¦ Wu, Z. (2025). _MiniMax-01: Scaling Foundation Models with Lightning Attention_. [https://arxiv.org/abs/2501.08313](https://arxiv.org/abs/2501.08313) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minimax01-3)
76.   Mistral AI. (2025). _Mistral Small 3.1_. [https://mistral.ai/news/mistral-small-3-1](https://mistral.ai/news/mistral-small-3-1)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mistralsmall-1)
77.   Moshkov, I., Hanley, D., Sorokin, I., Toshniwal, S., Henkel, C., Schifferer, B., Du, W., & Gitman, I. (2025). _AIMO-2 Winning Solution: Building State-of-the-Art Mathematical Reasoning Models with OpenMathReasoning dataset_. [https://arxiv.org/abs/2504.16891](https://arxiv.org/abs/2504.16891)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-moshkov2025aimo2winningsolutionbuilding-1)
78.   Muennighoff, N., Rush, A. M., Barak, B., Scao, T. L., Piktus, A., Tazi, N., Pyysalo, S., Wolf, T., & Raffel, C. (2025). _Scaling Data-Constrained Language Models_. [https://arxiv.org/abs/2305.16264](https://arxiv.org/abs/2305.16264)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-muennighoff2025scalingdataconstrainedlanguagemodels-1)
79.   Ni, J., Xue, F., Yue, X., Deng, Y., Shah, M., Jain, K., Neubig, G., & You, Y. (2024). _MixEval: Deriving Wisdom of the Crowd from LLM Benchmark Mixtures_. [https://arxiv.org/abs/2406.06565](https://arxiv.org/abs/2406.06565)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mixeval-1)
80.   Nrusimha, A., Brandon, W., Mishra, M., Shen, Y., Panda, R., Ragan-Kelley, J., & Kim, Y. (2025). _FlashFormer: Whole-Model Kernels for Efficient Low-Batch Inference_. [https://arxiv.org/abs/2505.22758](https://arxiv.org/abs/2505.22758)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nrusimha2025flashformerwholemodelkernelsefficient-1)
81.   Nvidia, :, Adler, B., Agarwal, N., Aithal, A., Anh, D. H., Bhattacharya, P., Brundyn, A., Casper, J., Catanzaro, B., Clay, S., Cohen, J., Das, S., Dattagupta, A., Delalleau, O., Derczynski, L., Dong, Y., Egert, D., Evans, E., â€¦ Zhu, C. (2024). _Nemotron-4 340B Technical Report_. [https://arxiv.org/abs/2406.11704](https://arxiv.org/abs/2406.11704)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nvidia2024nemotron4340btechnicalreport-1)
82.   NVIDIA, :, Basant, A., Khairnar, A., Paithankar, A., Khattar, A., Renduchintala, A., Malte, A., Bercovich, A., Hazare, A., Rico, A., Ficek, A., Kondratenko, A., Shaposhnikov, A., Bukharin, A., Taghibakhshi, A., Barton, A., Mahabaleshwarkar, A. S., Shen, A., â€¦ Chen, Z. (2025). _NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model_. [https://arxiv.org/abs/2508.14444](https://arxiv.org/abs/2508.14444)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nvidia2025nvidianemotronnano2-1)
83.   NVIDIA, :, Blakeman, A., Basant, A., Khattar, A., Renduchintala, A., Bercovich, A., Ficek, A., Bjorlin, A., Taghibakhshi, A., Deshmukh, A. S., Mahabaleshwarkar, A. S., Tao, A., Shors, A., Aithal, A., Poojary, A., Dattagupta, A., Buddharaju, B., Chen, B., â€¦ Chen, Z. (2025). _Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models_. [https://arxiv.org/abs/2504.03624](https://arxiv.org/abs/2504.03624)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-nemotronh-1)
84.   OLMo, T., Walsh, P., Soldaini, L., Groeneveld, D., Lo, K., Arora, S., Bhagia, A., Gu, Y., Huang, S., Jordan, M., Lambert, N., Schwenk, D., Tafjord, O., Anderson, T., Atkinson, D., Brahman, F., Clark, C., Dasigi, P., Dziri, N., â€¦ Hajishirzi, H. (2025). _2 OLMo 2 Furious_. [https://arxiv.org/abs/2501.00656](https://arxiv.org/abs/2501.00656) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-olmo2-3)
85.   OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., â€¦ Zoph, B. (2024). _GPT-4 Technical Report_. [https://arxiv.org/abs/2303.08774](https://arxiv.org/abs/2303.08774)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt4-1)
86.   Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). _Training language models to follow instructions with human feedback_. [https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-instructgpt-1)
87.   Penedo, G., KydlÃ­Äek, H., allal, L. B., Lozhkov, A., Mitchell, M., Raffel, C., Werra, L. V., & Wolf, T. (2024). _The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale_. [https://arxiv.org/abs/2406.17557](https://arxiv.org/abs/2406.17557)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb-1)
88.   Penedo, G., KydlÃ­Äek, H., SabolÄec, V., Messmer, B., Foroutan, N., Kargaran, A. H., Raffel, C., Jaggi, M., Werra, L. V., & Wolf, T. (2025). _FineWeb2: One Pipeline to Scale Them All â€“ Adapting Pre-Training Data Processing to Every Language_. [https://arxiv.org/abs/2506.20920](https://arxiv.org/abs/2506.20920) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-fineweb2-2)
89.   Peng, B., Goldstein, D., Anthony, Q., Albalak, A., Alcaide, E., Biderman, S., Cheah, E., Du, X., Ferdinan, T., Hou, H., Kazienko, P., GV, K. K., KocoÅ„, J., Koptyra, B., Krishna, S., Jr., R. M., Lin, J., Muennighoff, N., Obeid, F., â€¦ Zhu, R.-J. (2024). _Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence_. [https://arxiv.org/abs/2404.05892](https://arxiv.org/abs/2404.05892)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-peng2024eaglefinchrwkvmatrixvalued-1)
90.   Peng, B., Quesnelle, J., Fan, H., & Shippole, E. (2023). _YaRN: Efficient Context Window Extension of Large Language Models_. [https://arxiv.org/abs/2309.00071](https://arxiv.org/abs/2309.00071) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-yarn-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-yarn-2)
91.   Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., & Kong, L. (2021). _Random Feature Attention_. [https://arxiv.org/abs/2103.02143](https://arxiv.org/abs/2103.02143)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-peng2021randomfeatureattention-1)
92.   Petty, J., van Steenkiste, S., Dasgupta, I., Sha, F., Garrette, D., & Linzen, T. (2024). _The Impact of Depth on Compositional Generalization in Transformer Language Models_. [https://arxiv.org/abs/2310.19956](https://arxiv.org/abs/2310.19956) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-petty2024impactdepthcompositionalgeneralization-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-petty2024impactdepthcompositionalgeneralization-2)
93.   Polo, F. M., Weber, L., Choshen, L., Sun, Y., Xu, G., & Yurochkin, M. (2024). _tinyBenchmarks: evaluating LLMs with fewer examples_. [https://arxiv.org/abs/2402.14992](https://arxiv.org/abs/2402.14992)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-tinybenchmarks-1)
94.   Press, O., Smith, N. A., & Lewis, M. (2022). _Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation_. [https://arxiv.org/abs/2108.12409](https://arxiv.org/abs/2108.12409)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-alibi-1)
95.   Pyatkin, V., Malik, S., Graf, V., Ivison, H., Huang, S., Dasigi, P., Lambert, N., & Hajishirzi, H. (2025). _Generalizing Verifiable Instruction Following_. [https://arxiv.org/abs/2507.02833](https://arxiv.org/abs/2507.02833)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ifbench-1)
96.   Qin, Z., Han, X., Sun, W., Li, D., Kong, L., Barnes, N., & Zhong, Y. (2022). _The Devil in Linear Transformer_. [https://arxiv.org/abs/2210.10340](https://arxiv.org/abs/2210.10340)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qin2022devillineartransformer-1)
97.   Qin, Z., Yang, S., Sun, W., Shen, X., Li, D., Sun, W., & Zhong, Y. (2024). _HGRN2: Gated Linear RNNs with State Expansion_. [https://arxiv.org/abs/2404.07904](https://arxiv.org/abs/2404.07904)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qin2024hgrn2gatedlinearrnns-1)
98.   Qiu, Z., Huang, Z., Zheng, B., Wen, K., Wang, Z., Men, R., Titov, I., Liu, D., Zhou, J., & Lin, J. (2025). _Demons in the Detail: On Implementing Load Balancing Loss for Training Specialized Mixture-of-Expert Models_. [https://arxiv.org/abs/2501.11873](https://arxiv.org/abs/2501.11873)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qiu2025demonsdetailimplementingload-1)
99.   Qwen Team. (2025). _Qwen3-Next: Towards Ultimate Training & Inference Efficiency_. Alibaba Cloud. [https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list](https://qwen.ai/blog?id=4074cca80393150c248e508aa62983f9cb7d27cd&from=research.latest-advancements-list)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3next-1)
100.   Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., & others. (2019). Language models are unsupervised multitask learners. In _OpenAI blog_ (Vol. 1, p. 9).[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpt2-1)
101.   Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2024). _Direct Preference Optimization: Your Language Model is Secretly a Reward Model_. [https://arxiv.org/abs/2305.18290](https://arxiv.org/abs/2305.18290)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-dpo-1)
102.   Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., & Bowman, S. R. (2024). Gpqa: A graduate-level google-proof q&a benchmark. _First Conference on Language Modeling_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gpqa-1)
103.   RoziÃ¨re, B., Gehring, J., Gloeckle, F., Sootla, S., Gat, I., Tan, X. E., Adi, Y., Liu, J., Sauvestre, R., Remez, T., Rapin, J., Kozhevnikov, A., Evtimov, I., Bitton, J., Bhatt, M., Ferrer, C. C., Grattafiori, A., Xiong, W., DÃ©fossez, A., â€¦ Synnaeve, G. (2024). _Code Llama: Open Foundation Models for Code_. [https://arxiv.org/abs/2308.12950](https://arxiv.org/abs/2308.12950)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rozi%C3%A8re2024codellamaopenfoundation-1)
104.   Sennrich, R., Haddow, B., & Birch, A. (2016). _Neural Machine Translation of Rare Words with Subword Units_. [https://arxiv.org/abs/1508.07909](https://arxiv.org/abs/1508.07909)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-sennrich2016neuralmachinetranslationrare-1)
105.   Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y. K., Wu, Y., & Guo, D. (2024). _DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models_. [https://arxiv.org/abs/2402.03300](https://arxiv.org/abs/2402.03300)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-grpo-1)
106.   Shazeer, N. (2019). _Fast Transformer Decoding: One Write-Head is All You Need_. [https://arxiv.org/abs/1911.02150](https://arxiv.org/abs/1911.02150)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mqa-1)
107.   Shi, F., Suzgun, M., Freitag, M., Wang, X., Srivats, S., Vosoughi, S., Chung, H. W., Tay, Y., Ruder, S., Zhou, D., Das, D., & Wei, J. (2022). _Language Models are Multilingual Chain-of-Thought Reasoners_. [https://arxiv.org/abs/2210.03057](https://arxiv.org/abs/2210.03057)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mgsm-1)
108.   Shukor, M., Aubakirova, D., Capuano, F., Kooijmans, P., Palma, S., Zouitine, A., Aractingi, M., Pascal, C., Russi, M., Marafioti, A., Alibert, S., Cord, M., Wolf, T., & Cadene, R. (2025). _SmolVLA: A Vision-Language-Action Model for Affordable and Efficient Robotics_. [https://arxiv.org/abs/2506.01844](https://arxiv.org/abs/2506.01844)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smolvla-1)
109.   Singh, S., Romanou, A., Fourrier, C., Adelani, D. I., Ngui, J. G., Vila-Suero, D., Limkonchotiwat, P., Marchisio, K., Leong, W. Q., Susanto, Y., Ng, R., Longpre, S., Ko, W.-Y., Ruder, S., Smith, M., Bosselut, A., Oh, A., Martins, A. F. T., Choshen, L., â€¦ Hooker, S. (2025). _Global MMLU: Understanding and Addressing Cultural and Linguistic Biases in Multilingual Evaluation_. [https://arxiv.org/abs/2412.03304](https://arxiv.org/abs/2412.03304)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-globalmmlu-1)
110.   Sirdeshmukh, V., Deshpande, K., Mols, J., Jin, L., Cardona, E.-Y., Lee, D., Kritz, J., Primack, W., Yue, S., & Xing, C. (2025). _MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs_. [https://arxiv.org/abs/2501.17399](https://arxiv.org/abs/2501.17399)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-multichallenge-1)
111.   Smith, L. N., & Topin, N. (2018). _Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates_. [https://arxiv.org/abs/1708.07120](https://arxiv.org/abs/1708.07120)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-smith2018superconvergencefasttrainingneural-1)
112.   Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., & Liu, Y. (2023). _RoFormer: Enhanced Transformer with Rotary Position Embedding_. [https://arxiv.org/abs/2104.09864](https://arxiv.org/abs/2104.09864)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rope-1)
113.   Sun, Y., Dong, L., Zhu, Y., Huang, S., Wang, W., Ma, S., Zhang, Q., Wang, J., & Wei, F. (2024). _You Only Cache Once: Decoder-Decoder Architectures for Language Models_. [https://arxiv.org/abs/2405.05254](https://arxiv.org/abs/2405.05254)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-sun2024cacheoncedecoderdecoderarchitectures-1)
114.   Takase, S., Kiyono, S., Kobayashi, S., & Suzuki, J. (2025). _Spike No More: Stabilizing the Pre-training of Large Language Models_. [https://arxiv.org/abs/2312.16903](https://arxiv.org/abs/2312.16903)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-takase2025spikemorestabilizingpretraining-1)
115.   Team, 5, Zeng, A., Lv, X., Zheng, Q., Hou, Z., Chen, B., Xie, C., Wang, C., Yin, D., Zeng, H., Zhang, J., Wang, K., Zhong, L., Liu, M., Lu, R., Cao, S., Zhang, X., Huang, X., Wei, Y., â€¦ Tang, J. (2025). _GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models_. [https://arxiv.org/abs/2508.06471](https://arxiv.org/abs/2508.06471)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-glm45-1)
116.   team, F. C., Copet, J., Carbonneaux, Q., Cohen, G., Gehring, J., Kahn, J., Kossen, J., Kreuk, F., McMilin, E., Meyer, M., Wei, Y., Zhang, D., Zheng, K., Armengol-EstapÃ©, J., Bashiri, P., Beck, M., Chambon, P., Charnalia, A., Cummins, C., â€¦ Synnaeve, G. (2025). _CWM: An Open-Weights LLM for Research on Code Generation with World Models_. [https://arxiv.org/abs/2510.02387](https://arxiv.org/abs/2510.02387)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-cwm-1)
117.   Team, G., Kamath, A., Ferret, J., Pathak, S., Vieillard, N., Merhej, R., Perrin, S., Matejovicova, T., RamÃ©, A., RiviÃ¨re, M., Rouillard, L., Mesnard, T., Cideron, G., bastien Jean-Grill, Ramos, S., Yvinec, E., Casbon, M., Pot, E., Penchev, I., â€¦ Hussenot, L. (2025). _Gemma 3 Technical Report_. [https://arxiv.org/abs/2503.19786](https://arxiv.org/abs/2503.19786)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gemma3-1)
118.   Team, K., Bai, Y., Bao, Y., Chen, G., Chen, J., Chen, N., Chen, R., Chen, Y., Chen, Y., Chen, Y., Chen, Z., Cui, J., Ding, H., Dong, M., Du, A., Du, C., Du, D., Du, Y., Fan, Y., â€¦ Zu, X. (2025). _Kimi K2: Open Agentic Intelligence_. [https://arxiv.org/abs/2507.20534](https://arxiv.org/abs/2507.20534) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-kimik2-3)
119.   Team, L., Han, B., Tang, C., Liang, C., Zhang, D., Yuan, F., Zhu, F., Gao, J., Hu, J., Li, L., Li, M., Zhang, M., Jiang, P., Jiao, P., Zhao, Q., Yang, Q., Shen, W., Yang, X., Zhang, Y., â€¦ Zhou, J. (2025). _Every Attention Matters: An Efficient Hybrid Architecture for Long-Context Reasoning_. [https://arxiv.org/abs/2510.19338](https://arxiv.org/abs/2510.19338)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-lingteam2025attentionmattersefficienthybrid-1)
120.   Team, L., Zeng, B., Huang, C., Zhang, C., Tian, C., Chen, C., Jin, D., Yu, F., Zhu, F., Yuan, F., Wang, F., Wang, G., Zhai, G., Zhang, H., Li, H., Zhou, J., Liu, J., Fang, J., Ou, J., â€¦ He, Z. (2025). _Every FLOP Counts: Scaling a 300B Mixture-of-Experts LING LLM without Premium GPUs_. [https://arxiv.org/abs/2503.05139](https://arxiv.org/abs/2503.05139)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ling15-1)
121.   Team, M., Xiao, C., Li, Y., Han, X., Bai, Y., Cai, J., Chen, H., Chen, W., Cong, X., Cui, G., Ding, N., Fan, S., Fang, Y., Fu, Z., Guan, W., Guan, Y., Guo, J., Han, Y., He, B., â€¦ Sun, M. (2025). _MiniCPM4: Ultra-Efficient LLMs on End Devices_. [https://arxiv.org/abs/2506.07900](https://arxiv.org/abs/2506.07900)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-minicpm4-1)
122.   Tian, C., Chen, K., Liu, J., Liu, Z., Zhang, Z., & Zhou, J. (2025). _Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models_. [https://arxiv.org/abs/2507.17702](https://arxiv.org/abs/2507.17702) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-antgroup-3)
123.   Toshniwal, S., Moshkov, I., Narenthiran, S., Gitman, D., Jia, F., & Gitman, I. (2024). _OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset_. [https://arxiv.org/abs/2402.10176](https://arxiv.org/abs/2402.10176)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-toshniwal2024openmathinstruct118millionmath-1)
124.   Tunstall, L., Beeching, E., Lambert, N., Rajani, N., Rasul, K., Belkada, Y., Huang, S., von Werra, L., Fourrier, C., Habib, N., Sarrazin, N., Sanseviero, O., Rush, A. M., & Wolf, T. (2023). _Zephyr: Direct Distillation of LM Alignment_. [https://arxiv.org/abs/2310.16944](https://arxiv.org/abs/2310.16944)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-zephyr-1)
125.   Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., & Polosukhin, I. (2023). _Attention Is All You Need_. [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-transformer-3)
126.   Waleffe, R., Byeon, W., Riach, D., Norick, B., Korthikanti, V., Dao, T., Gu, A., Hatamizadeh, A., Singh, S., Narayanan, D., Kulshreshtha, G., Singh, V., Casper, J., Kautz, J., Shoeybi, M., & Catanzaro, B. (2024). _An Empirical Study of Mamba-based Language Models_. [https://arxiv.org/abs/2406.07887](https://arxiv.org/abs/2406.07887)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-waleffe2024empiricalstudymambabasedlanguage-1)
127.   Wang, B., & Komatsuzaki, A. (2021). _GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model_. [https://github.com/kingoflolz/mesh-transformer-jax](https://github.com/kingoflolz/mesh-transformer-jax)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-gptj-1)
128.   Wei, J., Karina, N., Chung, H. W., Jiao, Y. J., Papay, S., Glaese, A., Schulman, J., & Fedus, W. (2024). Measuring short-form factuality in large language models. _arXiv Preprint arXiv:2411.04368_.[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-simpleqa-1)
129.   Wen, K., Hall, D., Ma, T., & Liang, P. (2025). _Fantastic Pretraining Optimizers and Where to Find Them_. [https://arxiv.org/abs/2509.02046](https://arxiv.org/abs/2509.02046) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wen2025fantasticpretrainingoptimizers-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-wen2025fantasticpretrainingoptimizers-2)
130.   Xie, S. M., Pham, H., Dong, X., Du, N., Liu, H., Lu, Y., Liang, P., Le, Q. V., Ma, T., & Yu, A. W. (2023). _DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining_. [https://arxiv.org/abs/2305.10429](https://arxiv.org/abs/2305.10429)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-xie2023doremioptimizingdatamixtures-1)
131.   Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., â€¦ Ma, H. (2023a). _Effective Long-Context Scaling of Foundation Models_. [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-xiong2023effectivelongcontextscalingfoundation-1)
132.   Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., â€¦ Ma, H. (2023b). _Effective Long-Context Scaling of Foundation Models_. [https://arxiv.org/abs/2309.16039](https://arxiv.org/abs/2309.16039)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-ropeabf-1)
133.   Xu, H., Peng, B., Awadalla, H., Chen, D., Chen, Y.-C., Gao, M., Kim, Y. J., Li, Y., Ren, L., Shen, Y., Wang, S., Xu, W., Gao, J., & Chen, W. (2025). _Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math_. [https://arxiv.org/abs/2504.21233](https://arxiv.org/abs/2504.21233)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-phi4reasoning-1)
134.   Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., Zheng, C., Liu, D., Zhou, F., Huang, F., Hu, F., Ge, H., Wei, H., Lin, H., Tang, J., â€¦ Qiu, Z. (2025). _Qwen3 Technical Report_. [https://arxiv.org/abs/2505.09388](https://arxiv.org/abs/2505.09388) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-2), [3](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen3-3)
135.   Yang, A., Yu, B., Li, C., Liu, D., Huang, F., Huang, H., Jiang, J., Tu, J., Zhang, J., Zhou, J., Lin, J., Dang, K., Yang, K., Yu, L., Li, M., Sun, M., Zhu, Q., Men, R., He, T., â€¦ Zhang, Z. (2025). _Qwen2.5-1M Technical Report_. [https://arxiv.org/abs/2501.15383](https://arxiv.org/abs/2501.15383) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1million-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-qwen1million-2)
136.   Yang, B., Venkitesh, B., Talupuru, D., Lin, H., Cairuz, D., Blunsom, P., & Locatelli, A. (2025). _Rope to Nope and Back Again: A New Hybrid Attention Strategy_. [https://arxiv.org/abs/2501.18795](https://arxiv.org/abs/2501.18795) back: [1](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rnope-1), [2](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-rnope-2)
137.   Yang, G., & Hu, E. J. (2022). _Feature Learning in Infinite-Width Neural Networks_. [https://arxiv.org/abs/2011.14522](https://arxiv.org/abs/2011.14522)[](https://huggingfacetb-smol-training-playbook.hf.space/#refctx-bib-mup-1)
